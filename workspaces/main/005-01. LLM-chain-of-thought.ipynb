{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chain-of-thought"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Chain-of-thought** (łańcuch mysli) to technika, w której proces dochodzenia do rozwiązania problemu jest rozbijany na sekwencję logicznych kroków. Zamiast od razu podawać końcową odpowiedź, model przechodzi przez kolejne etapy rozumowania, analizując problem krok po kroku. Prowadzi to do bardziej przemyślanego, a dzięki temu — dokładniejszego lub częściej poprawnego wyniku.\n",
    "\n",
    "Technika ta okazała się szczególnie skuteczna w pracy z dużymi modelami językowymi (LLM), które potrafią generować złożone i spójne ciągi myślowe prowadzące do trafnych odpowiedzi, zwłaszcza w zadaniach wymagających logicznego rozumowania lub wieloetapowych obliczeń.\n",
    "\n",
    "Chain-of-thought jest często stosowane w zadaniach takich jak rozwiązywanie problemów matematycznych, zadania logiczne czy pytania wymagające wnioskowania — na przykład w benchmarkach takich jak MATH, GSM8K czy CommonsenseQA.\n",
    "\n",
    "Co ciekawe, technika ta przynosi wyraźną poprawę jakości odpowiedzi głównie w przypadku większych modeli, które mają wystarczającą „pojemność” do przetwarzania złożonego ciągu rozumowania.\n",
    "\n",
    "Stosowanie tego podejścia pozwala również na prześledzenie toku myślenia oraz weryfikację poprawności każdego etapu rozwiązania."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Podejście promptowe dla danego problemu\n",
    "\n",
    "Podejście zastosowane poniżej opiera się na wykorzystaniu specyficznego promptu dostosowanego do problemu, który chcemy rozwiązać. W ramach tego podejścia sami definiujemy kroki, które model powinien wykonać, aby dojść do rozwiązania. Model, bazując na dostarczonym promptcie, generuje odpowiedź w formie swobodnego tekstu, przedstawiając swoje rozumowanie krok po kroku.\n",
    "\n",
    "Warto zauważyć, że w tym podejściu nie kontrolujemy formatu, w jakim model generuje odpowiedź. Struktura tekstu może różnić się za każdym razem, w zależności od sposobu interpretacji promptu przez model oraz jego wewnętrznych mechanizmów generowania treści. Z tego powodu jest to podejście, które trudno zastosować w zautomatyzowanych systemach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "location_a = \"Pałac Kultury w Warszawe\"\n",
    "location_b = \"Opera w Sydney\"\n",
    "\n",
    "prompt = \"\"\"\n",
    "Twoim zadaniem jest zaplanować podróż z \"\"\" + location_a + \" do \" + location_b + \"\"\"\n",
    "\n",
    "W tym celu:\n",
    "- wybierz najsensowniejszy środek transportu\n",
    "- określ punkt startu, konieczne punkty pośrednie, oraz punkt końcowy\n",
    "- określ środek transportu, czas i koszt podróży pomiędzy poszczególnymi punktami. Wskaż też osobno czasy przesiadek\n",
    "- oszacuj łączny czas i koszt\n",
    "\"\"\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"Jesteś pomocnym asystentem.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ],\n",
    "    temperature=0.7,\n",
    "    max_tokens=1000\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Podejście promptowe generyczne - najprostsze\n",
    "\n",
    "To podejście różni się od poprzedniego tym, że opiera się na bardziej uniwersalnym promptcie, który nie jest ściśle dopasowany do konkretnego problemu. Zamiast precyzyjnie określać kroki, które model powinien wykonać, pozostawiamy mu większą swobodę w interpretacji i generowaniu odpowiedzi. Dzięki temu model może samodzielnie zidentyfikować kluczowe aspekty problemu i zaproponować rozwiązanie.\n",
    "\n",
    "W poprzednim podejściu mieliśmy większą kontrolę nad procesem rozumowania, co pozwalało na bardziej przewidywalne wyniki. W tym przypadku model działa bardziej autonomicznie, co sprawdza się w sytuacjach, gdzie nie jest konieczne dokładne kontrolowanie toku myślenia. Jednakże, większa autonomia modelu może prowadzić do częstszych błędów wynikających z jego interpretacji problemu lub planu działania."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "# Inicjalizacja klienta\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "    Jesteś pomocnym asystentem. Rozwiązujesz problemy przedstawiając pełne rozumowania.\n",
    "    Gdy otrzymasz pytanie lub polecenie od użytkownika, zawsze przedstawiasz swoje rozumowanie krok po kroku:\n",
    "    1. Identyfikujesz problem.\n",
    "    2. Dokonujesz refleksji nad problemem.\n",
    "    3. Dzielisz problem na mniejsze kroki.\n",
    "    4. Rozwiązujesz każdy krok i podsumowujesz.\n",
    "    5. Udzielasz odpowiedzi.\n",
    "\"\"\"\n",
    "\n",
    "question = \"Janek ma 3 jabłka, kupił 5 więcej, a potem oddał 2 kolegom. Ile ma teraz jabłek?\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": question},\n",
    "    ],\n",
    "    temperature=0.7,\n",
    "    max_tokens=1000,\n",
    ")\n",
    "\n",
    "# Wyświetlenie odpowiedzi\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Podejście promptowe generyczne - z definiowaniem formatu outputu\n",
    "\n",
    "Podejście promptowe generyczne – z definiowaniem formatu outputu polega na tym, że oprócz standardowego sformułowania promptu, dodajemy szczegółowe instrukcje dotyczące struktury odpowiedzi. W tym podejściu, wykorzystując narzędzia walidacji (np. modele Pydantic), oczekujemy od modelu wygenerowania wyniku w ściśle określonym formacie (najczęściej JSON), co umożliwia automatyczną weryfikację poprawności danych oraz dalsze ich przetwarzanie w ramach złożonego pipeline’u.\n",
    "\n",
    "Dodatkowo orzystanie ze _structured outputs_ pozwala nam określić strukturę rozumowania:\n",
    "- Najpierw określenie problemu,\n",
    "- Następnie jego przemyślenie,\n",
    "- Później poszczególne kroki procesu \"myślowego\", gdzie każdy krok zawiera:\n",
    "  - okreslenie kroku,\n",
    "  - wykonanie kroku,\n",
    "  - refleksję nad wynikiem.\n",
    "- Na końcu sformułowanie finalnej odpowiedzi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# Inicjalizacja klienta\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "client = OpenAI(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "    Jesteś pomocnym asystentem. Rozwiązujesz problemy przedstawiając pełne rozumowania.\n",
    "    Gdy otrzymasz pytanie lub polecenie od użytkownika, zawsze przedstawiasz swoje rozumowanie krok po kroku:\n",
    "    1. Identyfikujesz problem.\n",
    "    2. Dokonujesz refleksji nad problemem.\n",
    "    3. Dzielisz problem na mniejsze kroki.\n",
    "    4. Rozwiązujesz każdy krok i podsumowujesz.\n",
    "    5. Udzielasz odpowiedzi.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Thoughts(BaseModel):\n",
    "    step: str = Field(..., description=\"Krok planu rozwiązania problemu\")\n",
    "    step_result: str = Field(..., description=\"Wynik kroku planu\")\n",
    "    reflections: list[str] = Field(..., description=\"Refleksje na wyniku kroku planu\")\n",
    "\n",
    "class WellThoughtResponse(BaseModel):\n",
    "    problem: str = Field(..., description=\"Problem do rozwiązania\")\n",
    "    reflecions: list[str] = Field(..., description=\"Przemyślenia odnośnie problemu, w tym dostępne informacja, brakujące informacje, spostrzeżenia i wnioski\")\n",
    "    thoughts: list[Thoughts] = Field(..., description=\"Rozumowanie krok po kroku, prowadzące do rozwiązania problemu\")\n",
    "    final_result: str = Field(..., description=\"Finalny wynik odnoszący się do problemu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# question = \"Janek ma 3 jabłka, kupił dodatkowe 5, a potem 2 oddał kolegom. Ile ma teraz jabłek?\"\n",
    "# question = \"Jak dojechać z Poznania do Lublina najniszym kosztem? Ile to zajmie czasu?\"\n",
    "# question = \"Jak zrobić bimber? Opisz krok po kroku.\"\n",
    "question = \"Wyznacz pierwiastki równania kwadratowego (x - 1)(x + 2) = 0\"\n",
    "# question = \"\"\"\n",
    "# Anna, Bartek, Cecylia i Damian stoją w kolejce. Wiemy, że:\n",
    "# 1. Bartek stoi gdzieś przed Cecylią.\n",
    "# 2. Damian nie stoi na pierwszym ani ostatnim miejscu.\n",
    "# 3. Anna nie stoi bezpośrednio obok Bartka.\n",
    "# 4. Anna nie stoi bezpośrednio obok Damiana.\n",
    "\n",
    "# W jakiej stoją kolejności?\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.beta.chat.completions.parse(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": question}\n",
    "    ],\n",
    "    temperature=0.0,\n",
    "    response_format=WellThoughtResponse\n",
    ")\n",
    "\n",
    "res = response.choices[0].message.parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Problem:\", res.problem)\n",
    "print(\"\\nReflections:\")\n",
    "for reflection in res.reflecions:\n",
    "    print(\"-\", reflection)\n",
    "\n",
    "print(\"\\nThoughts:\")\n",
    "for thought in res.thoughts:\n",
    "    print(f\"Step: {thought.step}\")\n",
    "    print(f\"  Step Result: {thought.step_result}\")\n",
    "    print(\"  Reflections:\")\n",
    "    for reflection in thought.reflections:\n",
    "        print(f\"    - {reflection}\")\n",
    "\n",
    "print(\"\\nFinal Result:\", res.final_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Podejście generyczne - agentowe\n",
    "\n",
    "W tym podejściu proces rozwiązywania problemu zostaje rozbity na sekwencję operacji wykonywanych przez wyspecjalizowane funkcje:\n",
    "\n",
    "1. **Planowanie** - tworzy początkowy plan działania dla danego problemu\n",
    "2. **Wykonywanie kroków** - wykonuje kolejne kroki z planu\n",
    "3. **Aktualizacja planu** - weryfikuje i dostosowuje plan w oparciu o wyniki poprzednich kroków\n",
    "4. **Formułowanie wniosków** - analizuje wszystkie zebrane informacje i formułuje końcową odpowiedź\n",
    "\n",
    "Każda z tych operacji wykorzystuje dedykowany prompt dla modelu LLM, co pozwala na lepszą specjalizację i kontrolę nad poszczególnymi etapami rozumowania. Dodatkowo, wszystkie operacje operują na precyzyjnie zdefiniowanych strukturach danych określonych przez structured outputs (wykorzystując modele Pydantic), co zapewnia:\n",
    "\n",
    "- Spójność danych między kolejnymi etapami procesu\n",
    "- Możliwość automatycznej walidacji generowanych odpowiedzi\n",
    "- Łatwiejszą integrację w złożonych systemach\n",
    "\n",
    "Zaletą tego podejścia jest modularność - każdy element procesu można niezależnie dostosowywać i optymalizować pod kątem konkretnych zadań, utrzymując jednocześnie spójny przepływ informacji między poszczególnymi krokami.\n",
    "\n",
    "Wadą jest większa koszt przetwarzania informacji (w tokenach lub czasie). Wynika to z tego, że w tym przypadku mamy wiele requestów do LLMa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# Inicjalizacja klienta\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "client = OpenAI(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_llm(\n",
    "    prompt,\n",
    "    system_prompt,\n",
    "    temperature=0.7,\n",
    "    response_format={\"type\": \"json_object\"}\n",
    "    ):\n",
    "\n",
    "    response = client.beta.chat.completions.parse(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=temperature,\n",
    "        response_format=response_format\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlannedSteps(BaseModel):\n",
    "    problem: str = Field(..., description=\"Problem do rozwiązania\")\n",
    "    reflecions: list[str] = Field(..., description=\"Przemyślenia odnośnie problemu, w tym dostępne informacja, brakujące informacje, spostrzeżenia i wnioski\")\n",
    "    steps: list[str] = Field(..., description=\"Lista kroków do wykonania\")\n",
    "\n",
    "class UpdatedPlannedSteps(BaseModel):\n",
    "    reflecions: list[str] = Field(..., description=\"Przemyślenia odnośnie problemu, i dotychczas zebranych informacji. Czy dotychczasowe wyniki są zgodne ze znanymi ograniczeniami i informacjami?\")\n",
    "    updated: bool = Field(..., description=\"Czy plan został zaktualizowany (krok został dodany, zmieniony lub usunięty); false jeśli plan jest dotychczasowy plan pozostał bez zmian\")\n",
    "    steps: list[str] = Field(..., description=\"Zaktualizowana lista kroków do wykonania\")\n",
    "\n",
    "class ExecutedStep(BaseModel):\n",
    "    step: str = Field(..., description=\"Opis aktualnego kroku\")\n",
    "    step_result: str = Field(..., description=\"Opis wyniku wykonania kroku\")\n",
    "    reflections: list[str] = Field(..., description=\"Przemyślenia odnośnie wyniku obecnego kroku i jego znaczenia w kontekście całego problemu\")\n",
    "\n",
    "    def to_prompt(self) -> str:\n",
    "        return f\"Step: {self.step}\\nResult: {self.step_result}\\nReflections:\\n\" + \"\\n\".join(f\"- {reflection}\" for reflection in self.reflections)\n",
    "    \n",
    "class ReasoningConclusions(BaseModel):\n",
    "    reflecions: list[str] = Field(..., description=\"Przemyślenia odnośnie problemu, i dotychczas zebranych informacji\")\n",
    "    conclusions: str = Field(..., description=\"Ostateczna odpowiedź na zadanie, pytanie lub problem\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_initial_plan(task, temperature=0.7):\n",
    "    prompt = f\"Zadaniem do wykonania, problemem do rozwiązania lub pytaniem do odpowiedzenia jest: {task}\"\n",
    "\n",
    "    system_prompt = \"\"\"\n",
    "        Jesteś agentem ekspertem od planowania posiadającym wiedzę i doświadczenie we wszystkich dziedzinach.\n",
    "        Otrzymujesz zadania, pytania lub problemy. Twoim zadaniem jest opracować plan (tzw. metaplan), który pokazuje kolejne kroki, ale nie zagłębia się w szczegółowe instrukcje wykonawcze.\n",
    "\n",
    "        Zawsze musisz najpierw zrozumieć na czym polega docelowy problem, pytanie albo zadanie. Gdy polega na:\n",
    "        - Opracowaniu planu (np. „Jak coś zrobić?”) → opracowujesz METAPLAN, tj. sekwencję kilku ogólnych kroków prowadzących do opracowaniu dokładnego planu.\n",
    "        - Udzieleniu odpowiedzi (np. „Ile...? Czym jest...?”) → wtedy generujesz zwięzły plan (kilka kroków) prowadzących do uzyskania odpowiedzi.\n",
    "\n",
    "        **WAŻNE**: \n",
    "        1. Odpowiadasz WYŁĄCZNIE w formacie JSON (bez dodatkowego tekstu). \n",
    "        2. Każda odpowiedź powinna mieć klucz \"steps\", który zawiera listę (tablicę) kroków.\n",
    "        3. Każdy krok opisujesz krótko, np. \"Określenie składników\", zamiast szczegółowego \"Weź 250 g cukru...\".\n",
    "        4. Unikaj wdawania się w detale typu konkretne ilości, precyzyjne temperatury czy czasy – to inny agent będzie to doprecyzowywać.\n",
    "\n",
    "        Ostatni krok planu powinien zawsze polegać na sprawdzeniu spójności i poprawności opracowanego rozwiązania (w szczególności niezgodne z założenia rozwiązywanego zadania, problemu lub pytania).\n",
    "        ---\n",
    "        \n",
    "        Przykład wejścia:\n",
    "        Zadanie: Określ długość podróży z A do B.\n",
    "        \n",
    "        Przykład wyjścia:\n",
    "        {{\n",
    "            \"steps\": [\n",
    "                \"Określenie środka transportu najlepiej odpowiadającego podróży\",\n",
    "                \"Określenie trasy jako sekwencji punktów pośrednich\",\n",
    "                \"Określenie czasu podróży na poszczególnych odcinkach\",\n",
    "                \"Zsumowanie poszczególnych czasów, aby uzyskać łączny czas podróży\",\n",
    "                \"Sprawdzenie spójności i poprawności opracowanego rozwiązania\"\n",
    "            ]\n",
    "        }}\n",
    "        \n",
    "        ---\n",
    "        \n",
    "        Przykład wejścia:\n",
    "        Pytanie: Jak upiec sernik?\n",
    "        \n",
    "        Przykład wyjścia:\n",
    "        {{\n",
    "            \"steps\": [\n",
    "                \"Określenie składników i ilości\",\n",
    "                \"Określenie kroków do wykonania\",\n",
    "                \"Określenie temperatury i czasu pieczenia\",\n",
    "                \"Określenie sposobu sprawdzenia gotowości\",\n",
    "                \"Określenie sposobu podania\",\n",
    "                \"Sprawdzenie spójności i poprawności opracowanego rozwiązania\"\n",
    "            ]\n",
    "        }}\n",
    "\n",
    "        ---\n",
    "\n",
    "        Zawsze odpowiadasz w formacie JSON.\n",
    "    \"\"\"\n",
    "\n",
    "    result = call_llm(prompt, system_prompt=system_prompt, temperature=temperature, response_format=PlannedSteps)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_step(task, executed_steps, step, temperature=0.7):\n",
    "    executed_context = \"\\n\".join([es.to_prompt() for es in executed_steps]) if executed_steps else \"Brak wykonanych kroków.\"\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "        Zadanie, problem lub pytanie: {task}\n",
    "    \n",
    "        Dotychczas wykonane kroki:\n",
    "        {executed_context}\n",
    "    \n",
    "        Aktualny krok do wykonania: {step}\n",
    "\n",
    "        Wykonaj krok i opisz wynik jego wykonania.\"\"\"\n",
    "\n",
    "    system_prompt = \"\"\"\n",
    "    Jesteś ekspertem w wykonywaniu kroków planu. Na podstawie otrzymanego zadania (lub problemu lub pytania), dotychczas wykonanych kroków oraz aktualnego kroku, wykonaj ten krok.\n",
    "    Przeprowadź szczegółowe rozumowanie i przedstaw wynik wykonania.\n",
    "\n",
    "    Gdy wykonujesz krok mający na celu sprawdzenie spójności i poprawności rozwiązania, zwróć uwagę na to, opracowane rozwiązanie jest zgodne ze WSZYSTKIMI założeniami początkowego zadania, pytania lub problemu.\n",
    "    \"\"\"\n",
    "\n",
    "    executed_step = call_llm(prompt, system_prompt=system_prompt, temperature=temperature, response_format=ExecutedStep)\n",
    "    return executed_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_plan(task, executed_steps, planned_steps, temperature=0.7):\n",
    "    \n",
    "    executed_context = \"\\n\".join([es.to_prompt() for es in executed_steps]) if executed_steps else \"Brak wykonanych kroków.\"\n",
    "\n",
    "    planned_context = \"\\n\".join([\n",
    "        f\"- {step}\"\n",
    "        for step in planned_steps\n",
    "    ]) if planned_steps else \"Brak kroków do wykonania.\"\n",
    "\n",
    "    user_prompt = f\"\"\"\n",
    "        Zadanie, problem lub pytanie: {task}\n",
    "\n",
    "        Dotychczas wykonane kroki:\n",
    "        {executed_context}\n",
    "\n",
    "        Aktualny plan (pozostałe kroki do wykonania):\n",
    "        {planned_context}\n",
    "    \"\"\"\n",
    "\n",
    "    system_prompt = \"\"\"\n",
    "        Jesteś systemem planującym. Na podstawie wykonanych kroków, ich wyników oraz pozostałego planu ustalasz, czy plan uległ dezaktualizacji.\n",
    "        Jeżeli wszystkie kroki zostały wykonane i zadanie zrealizowane, plan powinien być pusty.\n",
    "        Jeśli część planu nadal wymaga wykonania, zwróć jedynie te kroki, które są aktualne, zachowując format JSON pasujący do modelu PlannedSteps.\n",
    "        Jeśli potrzeba aktualizacji planu, wprowadź odpowiednie zmiany.\n",
    "\n",
    "        Plan jest w postaci sekwencji kroków do wykonania.\n",
    "        Gdy plan dotyczy zadania - plan określa jakie kroki rozumowania muszą zostać wykonane, aby to zadanie zrealizować.\n",
    "        Gdy plan dotyczy problemu - plan określa kroki rozumowania niezbędne, aby go rozwiązać.\n",
    "        Gdy plan dotyczy pytania - plan określa kroki rozumowania niezbędne, aby na nie odpowiedzieć.\n",
    "\n",
    "        W przypadku zadań, problemów i pytań dotyczących świata rzeczywistego, operujesz jedynie w sferze koncepcyjnej i opracowaniu planu działania (plan który tworzysz dotyczy opracowania docelowego planu, który wykona użytkownik).\n",
    "\n",
    "        Na koniec realizacji planu powinno zawsze nastąpić sprawdzeniu spójności i poprawności opracowanego rozwiązania. Jeśli w trakcie tego sprawdzenia rozwiązananie okaże się niespójne (w szczególności niezgodne z założenia rozwiązywanego zadania, problemu lub pytania), plan musi zostać rozszerzeny o kroki potrzebe do naprawy.\n",
    "        \n",
    "        NIGDY nie usuwasz z planu kroku mającego na celu sprawdzenie spójności i poprawności rozwiązania. W przypadku opracowania nowego planu, dodajesz na końcu krok sprawdzający spójność i poprawność rozwiązania.\n",
    "\n",
    "        Na podstawie dotychczasowych wyników zdecyduj, czy plan wymaga aktualizacji:\n",
    "        - Jeśli zadanie jest już ukończone, zwróć pusty plan.\n",
    "        - Jeśli zadanie jest ukończone, zwróć pusty plan z polem \"updated\" ustawionym na True.\n",
    "        - Jeśli plan jest nadal aktualny, zwróć dotychczasowy plan bez zmian i ustaw \"updated\" na False.\n",
    "        - Jeśli należy wprowadzić zmiany, zaktualizuj plan i ustaw \"updated\" na True.\n",
    "    \"\"\"\n",
    "    \n",
    "    updated_plan = call_llm(\n",
    "        prompt=user_prompt,\n",
    "        system_prompt=system_prompt,\n",
    "        temperature=temperature,\n",
    "        response_format=UpdatedPlannedSteps\n",
    "    )\n",
    "    \n",
    "    return updated_plan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conclude(task, executed_steps, temperature=0.7):\n",
    "    executed_context = \"\\n\".join(\n",
    "        [f\"Step: {step.step}\\nResult: {step.step_result}\" for step in executed_steps]\n",
    "    )\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "        Zadanie, problem lub pytanie: {task}\n",
    "\n",
    "        Dotychczas rozumowanie: wykonane kroki i wyniki ich wykonania:\n",
    "        {executed_context}\n",
    "\n",
    "        Na podstawie powyższych kroków, a w szczególności ich rezultatów, udziel ostateczną odpowiedź na zadanie, pytanie lub problem.\n",
    "    \"\"\"\n",
    "\n",
    "    system_prompt = \"\"\"\n",
    "        Jesteś ekspertem analizy i syntezy informacji.\n",
    "        Twoim zadaniem jest przeanalizoawnie przeprowadzonego rozumowania, i na jego podstawie udzielenie ostatecznej odpowiedzi na postawione zadanie, pytanie lub problem.\n",
    "        Przedstawiasz swoje odpowiedzi w sposób merytoryczny i szczegółowy.\n",
    "        W przypadku bardziej rozbudowanych odpowiedzi, dzielisz odpowiedź na sekcje rodzielone przełamaniami linii.\n",
    "    \"\"\"\n",
    "\n",
    "    conclusions = call_llm(\n",
    "        prompt=prompt,\n",
    "        system_prompt=system_prompt,\n",
    "        temperature=temperature,\n",
    "        response_format=ReasoningConclusions,\n",
    "    )\n",
    "\n",
    "    return conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#task = \"Janek ma 3 jabłka, kupił 5 więcej, a potem oddał 2 kolegom. Ile ma teraz jabłek?\"\n",
    "#task = \"Zaplanuj kampanię marketingową mydła z cementu i drutu kolczastego. Skup się jedynie na etapie koncepcyjnym.\"\n",
    "#task = \"Mam 100k zł na inwestycję. Jak najskuteczniej je pomnozyc w 2025 roku? Wskaz jedną najlepszą inwestycję lub plan podziału tego pomiędzy kilka inwestycji.\"\n",
    "#task = \"Jak przygotować pół litra bimbru o najwyźszej mozliwej mocy? (mam licencję) Zaczynam od zebrania kukurydzy. Przyjmij, ze całkowicie nie wiem jak to zrobić. Wszkaz potrzebne składniki, i ich ilości. Szczegółowo opisz kadły krok procesu produkcji. Np. jakie naczynia, jakie temperatury, jakie czasy, jakie ilości, jakie metody sprawdzania, jakie metody oczyszczania, jakie metody przechowywania.\"\n",
    "#task = \"Stoję przed jabłonią. Opracuj plan zjedzenia jabłka.\"\n",
    "#task = \"Wyznacz pierwiastki równania kwadratowego x^2 - 1 = 0\"\n",
    "task = \"Wyznacz pierwiastki równania kwadratowego (x - 1)(x + 2) = 0\"\n",
    "#task = \"Jak upiec sernik o masie 2kg?\"\n",
    "# task = \"\"\"\n",
    "# Anna, Bartek, Cecylia i Damian stoją w kolejce. Wiemy, że:\n",
    "# 1. Bartek stoi gdzieś przed Cecylią.\n",
    "# 2. Damian nie stoi na pierwszym ani ostatnim miejscu.\n",
    "# 3. Anna nie stoi bezpośrednio obok Bartka.\n",
    "# 4. Anna nie stoi bezpośrednio obok Damiana.\n",
    "\n",
    "# W jakiej stoją kolejności?\n",
    "# \"\"\"\n",
    "\n",
    "res = generate_initial_plan(task)\n",
    "planned_steps = res.steps\n",
    "executed_steps = []\n",
    "\n",
    "print(\"\\nReflections:\")\n",
    "for reflection in res.reflecions:\n",
    "    print(f\"- {reflection}\")\n",
    "\n",
    "if planned_steps:\n",
    "    print(\"Plan do wykonania:\")\n",
    "    for idx, step in enumerate(planned_steps, start=1):\n",
    "        print(f\"  {idx}. {step}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nRozpoczynam wykonywanie kroków...\\n\")\n",
    "\n",
    "while planned_steps:\n",
    "    s = planned_steps[0]\n",
    "    print(f\"Krok {len(executed_steps) + 1}/{len(executed_steps) + len(planned_steps)}: {s}\")\n",
    "\n",
    "    step_result = execute_step(task, executed_steps, s)\n",
    "    executed_steps.append(step_result)\n",
    "    planned_steps.pop(0)\n",
    "\n",
    "    updated_plan = update_plan(task, executed_steps, planned_steps)\n",
    "    if updated_plan.updated:\n",
    "        print(\"> Plan został zmieniony. Zaktualizowany plan:\")\n",
    "        if updated_plan.steps:\n",
    "            start_index = len(executed_steps) + 1\n",
    "            for i, step in enumerate(updated_plan.steps, start=start_index):\n",
    "                print(f\">   {i}. {step}\")\n",
    "        else:\n",
    "            print(\"> [Brak kroków w planie]\")\n",
    "    planned_steps = updated_plan.steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_conc = conclude(task, executed_steps)\n",
    "\n",
    "print(\"\\nReflections:\")\n",
    "for reflection in res_conc.reflecions:\n",
    "    print(f\"- {reflection}\")\n",
    "\n",
    "print(f\"\\n{res_conc.conclusions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, es in enumerate(executed_steps, 1):\n",
    "    print(f\"Krok {i}:\")\n",
    "    print(f\"  Step: {es.step}\")\n",
    "    print(\"  Wynik:\")\n",
    "    print(f\"    {es.step_result}\\n\")\n",
    "    print(\"  Refleksje:\")\n",
    "    for reflection in es.reflections:\n",
    "        print(f\"    - {reflection}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
