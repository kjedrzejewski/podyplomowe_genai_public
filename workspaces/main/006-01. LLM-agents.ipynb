{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agenci LLM\n",
    "\n",
    "Agenci AI to systemy oparte na sztucznej inteligencji, które potrafią wykonywać określone zadania, takie jak przetwarzanie języka naturalnego, analiza danych czy automatyzacja procesów. Wykorzystują one zaawansowane algorytmy i modele, aby wspierać użytkowników w rozwiązywaniu problemów i podejmowaniu decyzji.\n",
    "\n",
    "W praktyce są one formą enkapsulacji logiki komunikacji z LLM-ami, co pozwala na łatwiejsze i bardziej efektywne korzystanie z ich możliwości."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic_ai import Agent, RunContext\n",
    "from pydantic_ai.models.openai import OpenAIModel\n",
    "from pydantic_ai.providers.openai import OpenAIProvider\n",
    "import nest_asyncio\n",
    "import os\n",
    "\n",
    "# Moduł `nest_asyncio` pozwala na zagnieżdżone użycie asyncio.run i innych pętli zdarzeń.\n",
    "# Jest to szczególnie przydatne w Jupyter Notebookach, gdzie pętla zdarzeń już działa.\n",
    "# Aby w Jupyter Notebook uruchomić `pydantic_ai`, należy zastosować `nest_asyncio`, \n",
    "# ponieważ `pydantic_ai` korzysta z pętli zdarzeń (event loop).\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Najprostszy przykład\n",
    "\n",
    "W praktyce te agenty robią to samo, co gdybyśmy wysłali prompta w trybie chatowym do LLMa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "provider = OpenAIProvider(api_key=api_key) # gdy nie podano `api_key`, zostanie użyta wartość z zmiennej środowiskowej `OPENAI_API_KEY`\n",
    "model = OpenAIModel('gpt-4o', provider=provider)\n",
    "\n",
    "agent = Agent(\n",
    "    model = model,\n",
    "    system_prompt='Bądź zwięzły, odpowiadaj jednym zdaniem.',\n",
    ")\n",
    "\n",
    "result = agent.run_sync('Skąd pochodzi wyrażenie \"hello world\"?')\n",
    "\n",
    "print(result.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(\n",
    "    model = 'openai:gpt-4o', # do uwierzytelnienia komunickacji wykorzystana zostanie zmienna środowiskowa OPENAI_API_KEY\n",
    "    system_prompt='Bądź zwięzły, odpowiadaj jednym zdaniem.',\n",
    ")\n",
    "\n",
    "result = agent.run_sync('Skąd pochodzi wyrażenie \"hello world\"?')\n",
    "\n",
    "print(result.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I jeszcze przykład z Gemini:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(\n",
    "    model = 'gemini-2.5-pro-exp-03-25',\n",
    "    system_prompt='Bądź zwięzły, odpowiadaj jednym zdaniem.',\n",
    ")\n",
    "\n",
    "result = agent.run_sync('Skąd pochodzi wyrażenie \"hello world\"?')\n",
    "\n",
    "print(result.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Przykład z toolem\n",
    "\n",
    "Agenty, tak samo jak _gołe_ LLMy, mogą korzystać z _function callingu_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "provider = OpenAIProvider(api_key=api_key)\n",
    "model = OpenAIModel('gpt-4o', provider=provider)\n",
    "\n",
    "agent = Agent(\n",
    "    model,\n",
    "    system_prompt='Jesteś pomocnym asystentem. Odpowiadaj na pytania i wykonuj polecenia korzystając z narzędzi.',\n",
    ")\n",
    "\n",
    "# Alternatywnie możemy skorzystać z Gemini 2.5 Pro jako modelu.\n",
    "# W tym celu musimy jedynie zmienić model w konstruktorze agenta (i mieć odpowiedni klucz API w zmiennej środowiskowej GEMINI_API_KEY).\n",
    "# agent = Agent(\n",
    "#     model = 'gemini-2.5-pro-exp-03-25',\n",
    "#     system_prompt='Jesteś pomocnym asystentem. Odpowiadaj na pytania i wykonuj polecenia korzystając z narzędzi.',\n",
    "# )\n",
    "\n",
    "@agent.tool\n",
    "def add_numbers(ctx: RunContext[None], a: int, b: int) -> int:\n",
    "    \"\"\"Narzędzie do dodawania dwóch liczb.\n",
    "    \n",
    "    Args:\n",
    "        a: Pierwsza liczba do dodania\n",
    "        b: Druga liczba do dodania\n",
    "    \n",
    "    Returns:\n",
    "        Suma dwóch liczb\n",
    "    \"\"\"\n",
    "    \n",
    "    return a + b\n",
    "\n",
    "result = agent.run_sync('Ile wynosi 123 + 456?')\n",
    "\n",
    "print(result.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Przykład z prognozą pogody\n",
    "\n",
    "Poważniejszy przykład korzystania z narzędzi / function callingu. Czyli ten sam przykład co był w [003-01. LLM-function_calling.ipynb](003-01.%20LLM-function_calling.ipynb), ale tym raze w Pydantic AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "provider = OpenAIProvider(api_key=api_key)\n",
    "model = OpenAIModel('gpt-4o', provider=provider)\n",
    "\n",
    "agent = Agent(\n",
    "    model,\n",
    "    system_prompt='Jesteś pomocnym asystentem. Odpowiadaj na pytania i wykonuj polecenia korzystając z narzędzi.',\n",
    ")\n",
    "\n",
    "# Ponownie - alternatywnie możemy skorzystać z Gemini 2.5 Pro jako modelu.\n",
    "# agent = Agent(\n",
    "#     model = 'gemini-2.5-pro-exp-03-25',\n",
    "#     system_prompt='Jesteś pomocnym asystentem. Odpowiadaj na pytania i wykonuj polecenia korzystając z narzędzi.',\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@agent.tool\n",
    "def get_geolocation(ctx: RunContext[None], location: str) -> str:\n",
    "    \"\"\"\n",
    "    Pobiera współrzędne geograficzne oraz dane lokalizacyjne.\n",
    "\n",
    "    Parametry:\n",
    "    location (str): Nazwa lokalizacji, dla której chcemy uzyskać współrzędne geograficzne.\n",
    "\n",
    "    Zwraca:\n",
    "    str: Dane lokalizacyjne w formacie JSON.\n",
    "    Struktura:\n",
    "    {\n",
    "        \"name\": \"pełna nazwa miejsca\",\n",
    "        \"latitude\": \"szerokość geograficzna\",\n",
    "        \"longitude\": \"długość geograficzna\"\n",
    "    }\n",
    "    \"\"\"\n",
    "    # Definiowanie endpointu i parametrów dla API OpenStreetMap Nominatim\n",
    "    geocode_endpoint = \"https://nominatim.openstreetmap.org/search\"\n",
    "    geocode_params = {\n",
    "        \"q\": location,\n",
    "        \"format\": \"json\"\n",
    "    }\n",
    "\n",
    "    # Nagłówek User-Agent jest wymagany do korzystania z tego API\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Python script\"\n",
    "    }\n",
    "\n",
    "    # Wykonanie zapytania\n",
    "    geocode_response = requests.get(geocode_endpoint, params=geocode_params, headers=headers)\n",
    "\n",
    "    # Wyciągnięcie współrzędnych geograficznych\n",
    "    geocode_data = geocode_response.json()\n",
    "    \n",
    "    # Jeśli nie znaleziono lokalizacji, zwracamy błąd\n",
    "    if not geocode_data:\n",
    "        return json.dumps({\"error\": \"Nie znaleziono lokalizacji\"})\n",
    "    \n",
    "    # Zwrócenie tylko nazwy miejsca, szerokości i długości geograficznej\n",
    "    simplified_data = {\n",
    "        \"name\": geocode_data[0][\"display_name\"],\n",
    "        \"latitude\": geocode_data[0][\"lat\"],\n",
    "        \"longitude\": geocode_data[0][\"lon\"]\n",
    "    }\n",
    "    # Konwersja słownika na format JSON\n",
    "    json_data = json.dumps(simplified_data)\n",
    "    \n",
    "    return json_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wind_direction(degrees):\n",
    "    \"\"\"\n",
    "    Konwertuje kierunek wiatru z stopni na nazwy kierunków.\n",
    "\n",
    "    Parametry:\n",
    "    degrees (float): Kierunek wiatru w stopniach.\n",
    "\n",
    "    Zwraca:\n",
    "    str: Nazwa kierunku wiatru.\n",
    "    \"\"\"\n",
    "    directions = ['N', 'NNE', 'NE', 'ENE', 'E', 'ESE', 'SE', 'SSE',\n",
    "                  'S', 'SSW', 'SW', 'WSW', 'W', 'WNW', 'NW', 'NNW']\n",
    "    index = int((degrees + 11.25) // 22.5) % 16\n",
    "    return directions[index]\n",
    "\n",
    "@agent.tool\n",
    "def get_current_weather(ctx: RunContext[None], latitude: float, longitude: float) -> str:\n",
    "    \"\"\"\n",
    "    Pobiera aktualne dane pogodowe dla podanych współrzędnych geograficznych.\n",
    "\n",
    "    Parametry:\n",
    "    latitude (float): Szerokość geograficzna.\n",
    "    longitude (float): Długość geograficzna.\n",
    "\n",
    "    Zwraca:\n",
    "    str: Dane pogodowe w formacie JSON.\n",
    "    Struktura:\n",
    "    {\n",
    "        \"temperature\": \"temperatura w stopniach Celsjusza\",\n",
    "        \"wind_speed\": \"prędkość wiatru w km/h\",\n",
    "        \"wind_direction\": \"kierunek wiatru (N, NE, E, SE, S, SW, W, NW)\",\n",
    "        \"is_day\": \"informacja czy jest dzień czy noc (day/night)\"\n",
    "    }\n",
    "    \"\"\"\n",
    "    # Zdefiniowanie endpointu i parametrów dla Open Meteo API\n",
    "    weather_endpoint = \"https://api.open-meteo.com/v1/forecast\"\n",
    "    weather_params = {\n",
    "        \"latitude\": latitude,\n",
    "        \"longitude\": longitude,\n",
    "        \"current_weather\": True\n",
    "    }\n",
    "\n",
    "    # Wykonanie requestu\n",
    "    weather_response = requests.get(weather_endpoint, params=weather_params)\n",
    "\n",
    "    # Wyciągnięcie informacji o pogodzie\n",
    "    weather_data = weather_response.json()\n",
    "\n",
    "    simplified_weather = {\n",
    "        \"temperature\": f\"{weather_data['current_weather']['temperature']} °C\",\n",
    "        \"wind_speed\": f\"{weather_data['current_weather']['windspeed']} km/h\",\n",
    "        \"wind_direction\": get_wind_direction(weather_data['current_weather']['winddirection']),\n",
    "        \"is_day\": \"day\" if weather_data['current_weather']['is_day'] else \"night\"\n",
    "    }\n",
    "\n",
    "    # Konwersja słownika na format JSON\n",
    "    json_data = json.dumps(simplified_weather)\n",
    "    \n",
    "    return json_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = agent.run_sync('Opisz jaka jest pogoda w Poznaniu. Czy powinienem wychodzić na spacer w stroju plażowym i okularach przeciwsłonecznych?')\n",
    "#result = agent.run_sync('Opisz jaka jest pogoda w 52.4 N, 16.92 E. Czy powinienem wychodzić na spacer w stroju plażowym i okularach przeciwsłonecznych?')\n",
    "\n",
    "print(result.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Przykład systemu wieloagentowego\n",
    "\n",
    "Przykład w którym agent do części czynności korzysta z innego agenta. \n",
    "\n",
    "Dodatkowo w przypładzie przekazujemy kontekst `usage` dzięki czemu na koniec możemy zobaczyć łączną liczbę wykorzystanych tokenów."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "provider = OpenAIProvider(api_key=api_key)\n",
    "model = OpenAIModel('gpt-4o', provider=provider)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Act(BaseModel):\n",
    "    title: str = Field(description=\"Tytuł aktu\")\n",
    "    scene_count: int = Field(description=\"Liczba scen w akcie\")\n",
    "    scene_descriptions: list[str] = Field(description=\"Opisy scen\")\n",
    "\n",
    "class Character(BaseModel):\n",
    "    name: str = Field(description=\"Nazwa postaci\")\n",
    "    description: str = Field(description=\"Opis postaci\")\n",
    "    role: str = Field(description=\"Rola postaci w scenariuszu, np. postać pierwszoplanowa, drugoplanowa, antagonista itp.\")\n",
    "\n",
    "class ScenarioOverview(BaseModel):\n",
    "    title: str = Field(description=\"Tytuł scenariusza\")\n",
    "    genres: list[str] = Field(description=\"Gatunki filmowe\")\n",
    "    header: str = Field(description=\"Jednozdaniowe streszczenie fabuły zawartej w scenariuszu\")\n",
    "    logline: str = Field(description=\"Kilka zdań, które określają głównego bohatera, wyzwanie które próbuje pokonać oraz dlaczego musi je pokonać\")\n",
    "    characters: list[Character] = Field(description=\"Lista postaci w scenariuszu\")\n",
    "    acts: list[Act] = Field(description=\"Lista aktów w filmie\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "character_agent = Agent(\n",
    "    model,\n",
    "    result_type=list[Character],\n",
    "    system_prompt='Jesteś doświadczonym scenarzystą. Specjalizujesz się w tworzeniu postaci do filmu w oparciu o ogólne założenia scenariusza.',\n",
    ")\n",
    "\n",
    "# Albo korzystając z Gemini 2.5 Pro jako modelu.\n",
    "# character_agent = Agent(\n",
    "#     model = 'gemini-2.5-pro-exp-03-25',\n",
    "#     result_type=list[Character],\n",
    "#     system_prompt='Jesteś doświadczonym scenarzystą. Specjalizujesz się w tworzeniu postaci do filmu w oparciu o ogólne założenia scenariusza.',\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "act_agent = Agent(\n",
    "    model,\n",
    "    result_type=Act,\n",
    "    system_prompt='Jesteś doświadczonym scenarzystą. Specjalizujesz się w tworzeniu zarysów aktów filmowych, w oparciu o ogólne założenia scenariusza i informacje o postaciach.',\n",
    ")\n",
    "\n",
    "# Albo korzystając z Gemini 2.5 Pro jako modelu.\n",
    "# act_agent = Agent(\n",
    "#     model = 'gemini-2.5-pro-exp-03-25',\n",
    "#     result_type=Act,\n",
    "#     system_prompt='Jesteś doświadczonym scenarzystą. Specjalizujesz się w tworzeniu zarysów aktów filmowych, w oparciu o ogólne założenia scenariusza i informacje o postaciach.',\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "scenario_agent_system_prompt=\"\"\"\n",
    "    Jesteś doświadczonym scenarzystą filmowym.\n",
    "    Twoim zadaniem jest stworzenie kompletnego zarysu scenariusza filmowego, dbając o spójność treści i formy.\n",
    "    W procesie tworzenia korzystasz z dedykowanych narzędzi do generowania listy postaci oraz zarysów aktów filmowych.\n",
    "\n",
    "    Na początku stwórz ogólny zarys filmu, uwzględniając jego gatunek, główny temat oraz kluczowe założenia fabularne.\n",
    "\n",
    "    Następnie, użyj narzędzia do stworzenia listy postaci, które będą odgrywać kluczowe role w filmie. Każda postać powinna mieć unikalne cechy, które wspierają fabułę.\n",
    "\n",
    "    Kolejnym krokiem jest opracowanie zarysów aktów filmowych. Dla każdego aktu:\n",
    "    - Przekaż ogólne założenia dotyczące filmu.\n",
    "    - Uwzględnij listę postaci wraz z ich krótkimi opisami.\n",
    "    - Podaj listę planowanych aktów z krótkimi założeniami fabularnymi.\n",
    "    - Dołącz streszczenie fabuły aktów już napisanych, szczególnie uwzględniając informacje mające wpływ na dalsze wydarzenia.\n",
    "    - Wyraźnie określ, który akt ma zostać opracowany, np. \"Twoim zadaniem jest utworzenie zarysu aktu <numer aktu> w oparciu o następujące założenia: <założenia>\".\n",
    "\n",
    "    Pamiętaj, aby każdy akt był spójny z poprzednimi i rozwijał fabułę w logiczny sposób, prowadząc do satysfakcjonującego zakończenia. Akty twórz w kolejności, zaczynając od aktu I, a kończąc na ostatnim akcie - akty pierwszy tworzysz po stworzeniu ogólnego zarysu filmu, a kolejne akty tworzysz po stworzeniu aktów poprzednich.\n",
    "\"\"\"\n",
    "\n",
    "scenario_agent = Agent(\n",
    "    model,\n",
    "    result_type=ScenarioOverview,\n",
    "    system_prompt=scenario_agent_system_prompt\n",
    ")\n",
    "\n",
    "# Albo korzystając z Gemini 2.5 Pro jako modelu.\n",
    "# scenario_agent = Agent(\n",
    "#     model = 'gemini-2.5-pro-exp-03-25',\n",
    "#     result_type=ScenarioOverview,\n",
    "#     system_prompt=scenario_agent_system_prompt,\n",
    "# )\n",
    "\n",
    "@scenario_agent.tool\n",
    "def create_characters(ctx: RunContext[None], assumptions: str) -> list[Character]:\n",
    "    \"\"\"\n",
    "    Tworzy listę postaci do filmu na podstawie podanych założeń scenariusza.\n",
    "    \n",
    "    Funkcja wykorzystuje character_agent do wygenerowania listy postaci, które\n",
    "    pasują do założeń scenariusza.\n",
    "    \n",
    "    Args:\n",
    "        ctx: Kontekst wykonania, używany przez framework pydantic_ai\n",
    "        assumptions: Założenia scenariusza opisujące ogólny zarys filmu\n",
    "        \n",
    "    Returns:\n",
    "        list[Character]: Lista obiektów typu Character, zawierająca wygenerowane postacie\n",
    "    \"\"\"\n",
    "\n",
    "    # dzięki temu możemy śledzić, czy narzędzie jest wykorzystywane\n",
    "    with open(\"aaa.txt\", \"a\") as file:\n",
    "        current_date = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        file.write(f\"[{current_date}][create_characters]: {assumptions}\\n\\n\")\n",
    "\n",
    "    characters = character_agent.run_sync(\n",
    "        f\"Utwórz listę postaci do filmu w oparciu o następujace założenia: {assumptions}\",\n",
    "        usage=ctx.usage # abyśmy mogli zobaczyć łączną liczbę wykorzystaych tokenów\n",
    "    ).data\n",
    "\n",
    "    return characters\n",
    "\n",
    "@scenario_agent.tool\n",
    "def create_act(ctx: RunContext[None], input_str: str) -> Act:\n",
    "    \"\"\"\n",
    "    Tworzy zarys aktu filmowego na podstawie tekstu zawierającego założenia scenariusza, informacje o postaciach oraz kontekst fabuły z poprzednich aktów\n",
    "    \n",
    "    Funkcja wykorzystuje act_agent do wygenerowania zarysu aktu filmowego zgodnego\n",
    "    z podanymi informacjami.\n",
    "    \n",
    "    Args:\n",
    "        ctx: Kontekst wykonania, używany przez framework pydantic_ai\n",
    "        input_str: Tekst zawierający wszystkie informacje potrzebne do utworzenia aktu,\n",
    "                   w tym założenia filmu oraz każdego aktu, informacje o postaciach oraz \n",
    "                   kontekst fabuły z poprzednich aktów, szczególnie informacje mające wpływ\n",
    "                   na dalsze wydarzenia (o ile to nie jest akt I)\n",
    "        \n",
    "    Returns:\n",
    "        Act: Obiekt typu Act, zawierający wygenerowany zarys aktu filmowego\n",
    "    \"\"\"\n",
    "    \n",
    "    # dzięki temu możemy śledzić, czy narzędzie jest wykorzystywane\n",
    "    with open(\"aaa.txt\", \"a\") as file:\n",
    "        current_date = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        file.write(f\"[{current_date}][create_act]: {input_str}\\n\\n\")\n",
    "\n",
    "    act = act_agent.run_sync(\n",
    "        f\"Utwórz akt filmowy w oparciu o następujace założenia (w tym listę postaci): {input_str}\",\n",
    "        usage=ctx.usage # abyśmy mogli zobaczyć łączną liczbę wykorzystaych tokenów\n",
    "    ).data\n",
    "\n",
    "    return act\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = scenario_agent.run_sync('Stwórz zarys scenariusza filmowego kolejnej części \"Samych Swoich\", ale w klimatach sci-fi.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import parse_nested_json\n",
    "import json\n",
    "\n",
    "parsed_data = parse_nested_json(res.data.model_dump_json())\n",
    "\n",
    "print(json.dumps(parsed_data, indent=4, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.usage()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent z pamięcią\n",
    "\n",
    "Przykład w którym agent zapamiętuje informacje w postaci listy faktów. W tym przypadku w postaci listy. Ale może też w tym celu skorzystać np. z bazy danych."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt=\"\"\"\n",
    "    Wspomagaj użytkownika, personalizując odpowiedzi na podstawie zapamiętanych informacji o jego preferencjach, wcześniejszych interakcjach i całej historii konwersacji.\n",
    "\n",
    "    # Instrukcje\n",
    "\n",
    "    - **Zapamiętywanie**: *ZAWSZE* używaj funkcji `remember_info()` do zachowywania informacji o:\n",
    "      - preferencjach użytkownika,\n",
    "      - prośbach i poleceniach,\n",
    "      - zadanych pytaniach,\n",
    "      - twoich odpowiedziach,\n",
    "      - całej dotychczasowej historii konwersacji.\n",
    "\n",
    "    - **Przywoływanie pamięci**: Stosuj `recall_memory()` przed udzieleniem odpowiedzi, aby mieć pełny kontekst interakcji.\n",
    "\n",
    "    - **Dostosowanie odpowiedzi**: Personalizuj odpowiedzi zgodnie z zapamiętanymi informacjami, biorąc pod uwagę preferencje użytkownika i jego styl komunikacji.\n",
    "\n",
    "    - **Dyskrecja**: Nie ujawniaj użytkownikowi szczegółowych informacji o pamiętanych interakcjach, chyba że zostaniesz o to poproszony.\n",
    "\n",
    "    *WAŻNE*: zawsze zapamiętuj wszystkie informacje o interakcjach z użytkownikiem, w tym całą historię konwersacji.\n",
    "\n",
    "    # Output Format\n",
    "\n",
    "    Odpowiedzi powinny być zwięzłe i zgodne z zapamiętanymi preferencjami użytkownika oraz odnosić się do wcześniejszych interkacji. Powinny być przedstawione w krótkim zdaniu lub akapicie, zależnie od prośby.\n",
    "\"\"\"\n",
    "\n",
    "# Agent oparty na OpenAI GPT-4o ma opory w zapisywaniu historii konwersacji\n",
    "# api_key = os.getenv('OPENAI_API_KEY')\n",
    "# provider = OpenAIProvider(api_key=api_key)\n",
    "# model = OpenAIModel('gpt-4o', provider=provider)\n",
    "\n",
    "# memory_agent = Agent(\n",
    "#     model,\n",
    "#     system_prompt=system_prompt\n",
    "# )\n",
    "\n",
    "# Gemini 2.5 Pro nie ma problemów z zapisywaniem historii konwersacji\n",
    "memory_agent = Agent(\n",
    "    model = 'gemini-2.5-pro-exp-03-25',\n",
    "    system_prompt=system_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definiujemy klasę pamięci agenta\n",
    "class Memory:\n",
    "    def __init__(self):\n",
    "        self.memory: list[str] = []\n",
    "\n",
    "    def remember(self, info: str):\n",
    "        self.memory.append(info)\n",
    "\n",
    "    def recall(self) -> list[str]:\n",
    "        return self.memory\n",
    "\n",
    "# Tworzymy instancję pamięci\n",
    "memory = Memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "@memory_agent.tool\n",
    "def remember_info(ctx: RunContext[None], info: str) -> str:\n",
    "    \"\"\"\n",
    "    Używasz aby zapamiętać informacje o:\n",
    "    - preferencjach użytkownika,\n",
    "    - prośbach, poleceniach,\n",
    "    - zadanych pytaniach\n",
    "    - i twoich odpowiedziach.\n",
    "\n",
    "    Parametry:\n",
    "    info (str): Zwięzła informacja, którą dowiedziałeś się lub wywnioskowałeś o użytkownika, albo o prośbie lub pytaniu użytkownika.\n",
    "\n",
    "    Zwraca:\n",
    "    str: Potwierdzenie dodania informacji do pamięci, oraz liczba informacji w pamięci.\n",
    "    \"\"\"\n",
    "    with open(\"aaa.txt\", \"a\") as file:\n",
    "        current_date = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        file.write(f\"[{current_date}][remember_info]: {info}\\n\\n\")\n",
    "\n",
    "    memory.remember(info)\n",
    "    return f\"Zapamiętano. Aktualna liczba informacji w pamięci: {len(memory.memory)}\"\n",
    "\n",
    "@memory_agent.tool\n",
    "def recall_memory(ctx: RunContext[None]) -> list[str]:\n",
    "    \"\"\"\n",
    "    Pozwala Ci przypomnieć sobie wszystkie zapamiętane informacje.\n",
    "\n",
    "    Zwraca:\n",
    "    list[str]: Lista zapamiętanych informacji.\n",
    "    \"\"\"\n",
    "    with open(\"aaa.txt\", \"a\") as file:\n",
    "        current_date = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        file.write(f\"[{current_date}][recall_memory]: zażądano\\n\\n\")\n",
    "\n",
    "    return memory.recall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(memory_agent.run_sync('Co o mnie wiesz?').data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(memory_agent.run_sync('Lubię placki').data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(memory_agent.run_sync('Co o mnie wiesz?').data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(memory_agent.run_sync('Po ile jest śnieg?').data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(memory_agent.run_sync('Gdy mówię o śniegu, to mam na myśli cukier puder').data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(memory_agent.run_sync('Po ile jest śnieg?').data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(memory_agent.run_sync('O czym rozmawialiśmy do tej pory?').data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(memory_agent.run_sync('Kim jestem?').data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# System wieloagentowy z pamięcią nieustrukturyzowaną\n",
    "\n",
    "Tym razem operujemy na pamięci w postaci pojedynczego ciągu znaków. Mamy dwóch agentów - jednego od zarządzania pamięcią, drugiego od komunikacji z użytkownikiem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "provider = OpenAIProvider(api_key=api_key)\n",
    "model = OpenAIModel('gpt-4o', provider=provider)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_agent_system_prompt=\"\"\"\n",
    "    Zarządzasz pamięcią w postaci stringa, która przechowuje informacje o użytkowniku.\n",
    "    Otrzymujesz obecny stan wiedzy o użytkowniku wspieranym przez agenta-asystenta, i masz za zadanie zaktualizować ten stan wiedzy w oparciu o nowe interkacje.\n",
    "    Odpowiadasz za pomocą stringa, który jest nowym stanem wiedzy (uwzględnia zarówno wcześniejsze informacje, oraz te wywnioskowane w oparciu o nowe polecenie użytkownika).\n",
    "    Kiedy któreś z informacji przestają być aktualne, usuwasz je z pamięci.\n",
    "\n",
    "    PAMIĘTAJ: użytkownik komunikuje się z asystentem, a z nie z Tobą. Ty jedynie jesteś z boku, widzisz te interakcje i dostarczasz asystentowi informacje o użytkowniku. \n",
    "\"\"\"\n",
    "\n",
    "memory_agent = Agent(\n",
    "    model,\n",
    "    system_prompt=memory_agent_system_prompt\n",
    ")\n",
    "\n",
    "# Albo Gemini jako alternatywa\n",
    "# memory_agent = Agent(\n",
    "#     model = 'gemini-2.5-pro-exp-03-25',\n",
    "#     system_prompt=memory_agent_system_prompt\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assisstant_agent_system_prompt=\"\"\"\n",
    "        Jesteś pomocnym asystentem. Twoim zadaniem jest odpowiadać na pytania użytkownika, korzystając z informacji o użytkowniku dostarczonymi przez agenta zarządzającego pamięcią.\n",
    "        Nie zdradzasz użytkownikowi informacji o istnieniu agenta pamięciąwego. Z jego perspektywy, to Ty jesteś jedynym asystentem, i to Ty wszystko pamiętasz.\n",
    "        Odpowiadasz zwięźle i na temat. Zależnie od konktekstu odpowiadasz jednym zdaniem lub krótkim lub dłuższym akapitem.\n",
    "    \"\"\"\n",
    "\n",
    "assisstant_agent = Agent(\n",
    "    model,\n",
    "    system_prompt=assisstant_agent_system_prompt\n",
    ")\n",
    "\n",
    "# Albo Gemini jako alternatywa\n",
    "# assisstant_agent = Agent(\n",
    "#     model = 'gemini-2.5-pro-exp-03-25',\n",
    "#     system_prompt=assisstant_agent_system_prompt\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tym razem pamięć jest po prostu stringiem\n",
    "memory = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def interact(user_prompt: str) -> str:\n",
    "\n",
    "    global memory\n",
    "\n",
    "    new_memory_state = memory_agent.run_sync(f\"\"\"\n",
    "        Twój obecny stan wiedzy o użytkowniku to:\n",
    "        {memory}\n",
    "\n",
    "        Zaktualizuj ten stan wiedzy w oparciu o poniższe polecenie od użytkownika:\n",
    "        {user_prompt}                        \n",
    "    \"\"\").data\n",
    "\n",
    "    with open(\"aaa.txt\", \"a\") as file:\n",
    "        current_date = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        file.write(f\"[{current_date}][interact]: {new_memory_state}\\n\\n\")\n",
    "\n",
    "    response = assisstant_agent.run_sync(f\"\"\"\n",
    "        Agent zarządzający pamięcią dostarcza Ci następujący stan wiedzy o użytkowniku:\n",
    "        {memory}\n",
    "\n",
    "        Korzystając z niego i swojej własnej wiedzy, odpowiedz na poniższe polecenie od użytkownika:\n",
    "        {user_prompt}                        \n",
    "    \"\"\").data\n",
    "\n",
    "    memory = new_memory_state\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(interact('Co o mnie wiesz?'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(interact('Lubię placki'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(interact('Co o mnie wiesz?'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(interact('Po ile jest śnieg?'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(interact('Gdy mówię o śniegu, to mam na myśli cukier puder'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(interact('Po ile jest śnieg?'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(interact('O czym rozmawialiśmy do tej pory?'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(interact('Zmieniłem zdanie - zamiast placków wolę lody'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(interact('Co lubię jeść?'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(interact('Jestem pacyfistą.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(interact('Jak uważasz, czy chętnie walczyłbym w wojnie?'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(interact('Co o mnie wiesz?'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(interact('Ile to jest 2 + 2?'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(interact('Kierowca fiata ma brata, ale brat kierowcy fiata nie ma brata. Jak to możliwe?'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(interact('Co o mnie wiesz?'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(interact('Czy to Ty pamiętasz to wszystko o mnie?'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(interact('Mam 2 koty'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(interact('Adoptowałem kolejnego kota'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(interact('Mam psa'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(interact('Mam złotą rybkę'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(interact('ile mam zwierząt domowych i jakie?'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(interact('Wypisz w punktach co o mnie wiesz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
