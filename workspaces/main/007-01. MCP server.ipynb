{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Serwery Model Context Protocol\n",
    "\n",
    "Model Context Protocol (MCP) to mechanizm zapewniający LLMom dostęp do narzędzi i funkcji, oraz informacji z zewnętrznych systemów, które mogą one wykorzystywać przez mechanizm Function Calling. Przy czym, jako że jest to protokół ustandaryzowany, umożliwia on korzystanie z jednego serwera przez modele i asystentów od różnych dostawców.\n",
    "\n",
    "W tym notebooku mamy trzy różne podejścia do wykorzystania MCP:\n",
    "1. **Bezpośrednia komunikacja z OpenAI API i serwerem MCP** - implementacja niskopoziomowa\n",
    "2. **Pydantic AI** - połączenie agenta Pydantic AI z serwerem MCP\n",
    "3. **Open AI agents** - połączenie agenta Open AI Agents z serwerem MCP\n",
    "\n",
    "Każde podejście demonstruje, jak modele mogą wykorzystywać narzędzia / funkcje udostępniane przez serwer MCP do wykonywania zadań wykraczających poza ich standardowe możliwości.\n",
    "\n",
    "---\n",
    "\n",
    "Na końcu notebooka znajduje się również przykład niskopoziomowego korzystania z zasobów udostępnianych przez serwer MCP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Podejście niskopoziomowe\n",
    "\n",
    "Komunikację z serwerem MCP obsługujemy niskopoziomowo:\n",
    "- sami pobieramy listę narzędzi z serwera MCP\n",
    "- sami tłumaczymy opis dostępny funkcji z formatu MCP, na format oczekiwany przez modele Open AI\n",
    "- sami wykonujemy funkcje na serwerze MCP i przekazujemy wynik z powrotem do modelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mcp import ClientSession\n",
    "from mcp.client.sse import sse_client\n",
    "import json\n",
    "from openai import OpenAI\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adres serwera MCP\n",
    "mcp_url = \"http://mcp:8801/sse\"\n",
    "\n",
    "# Aby korzysztać z lokalnego serwera MCP, uruchom go w terminalu wspisując:\n",
    "# > export MCP_PORT=1234\n",
    "# > python mcp_scripts/math_functions.py \n",
    "# i odkomentuj poniższą linię\n",
    "# mcp_url = \"http://localhost:1234/sse\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def mcp_list_tools_lowlevel(mcp_url):\n",
    "    \"\"\"\n",
    "    Pobiera listę dostępnych narzędzi z serwera MCP.\n",
    "\n",
    "    Parametry:\n",
    "    mcp_url (str): URL serwera MCP.\n",
    "\n",
    "    Zwraca:\n",
    "    list: Lista dostępnych narzędzi w formacie zwracanym przez serwer MCP.\n",
    "    \"\"\"\n",
    "    # Nawiązanie połączenia SSE\n",
    "    async with sse_client(url=mcp_url) as (read, write):\n",
    "        # Utworzenie sesji klienta\n",
    "        async with ClientSession(read, write) as session:\n",
    "            # Inicjalizacja połączenia\n",
    "            await session.initialize()\n",
    "\n",
    "            # Pobranie listy dostępnych narzędzi\n",
    "            tools_response = await session.list_tools()\n",
    "\n",
    "            return tools_response.tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pobranie listy dostępnych narzędzu (wywołanie funkcji asynchronicznej)\n",
    "tools = await mcp_list_tools_lowlevel(mcp_url)\n",
    "\n",
    "# Jeśli chcesz uruchomić to w kontekście nie-asynchronicznym, możesz użyć asyncio.run()\n",
    "# Gdy uruchamiamy w Jupyter Notebooku, to nie jest konieczne, bo on już działa w pętli asynchronicznej\n",
    "# \n",
    "# import asyncio\n",
    "# tools = asyncio.run(mcp_list_tools_lowlevel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcp_tools_json = json.dumps([tool.model_dump() for tool in tools], indent=4)\n",
    "\n",
    "print(mcp_tools_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Format opisu narzędzi zwracany przez serwer MCP jest inny niż format oczekiwany przez modele od Open AI. Musimy więc ten opis przekonwertować. W tym przykładzie robimy to użzywając prompta. Jednak, w zastosowaniach produkcyjnych poprawniej jest zrobić to z wykorzystaniem dedykowanej logiki tłumaczącej jeden z formatów na drugi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "conversion_prompt = f\"\"\"\n",
    "    Przekonwertuj poniższy dokument JSON opisujący toole zgodnie z formatem Model Context Protocol na format zgodny z OpenAI Tools Schema.\n",
    "    \n",
    "    Opis narzędzii ma następujący format:\n",
    "    {{\n",
    "        \"tools\" = [\n",
    "            {{\n",
    "                \"type\": \"function\",\n",
    "                \"function\": {{... opis danej funckji ...}}\n",
    "            }},\n",
    "            {{\n",
    "                \"type\": \"function\",\n",
    "                \"function\": {{... opis danej funckji ...}}\n",
    "            }}\n",
    "        ]\n",
    "    }}\n",
    "\n",
    "    Pamiętaj o polach:\n",
    "    - function.parameters.required,\n",
    "    - function.parameters.additionalProperties\n",
    "    - function.strict\n",
    "\n",
    "    Pamiętaj, że w OpenAI Tools Schema parametry są opisane za pomocą 'type' i 'description'.\n",
    "    \n",
    "    Description dla pól możesz musieć wyciągnąć z opisu parametrów w głównym description opisu toola w Model Context Protocol.\n",
    "    ---\n",
    "    {mcp_tools_json}\n",
    "\"\"\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"Jesteś ekspertem od przetwarzania dokumentów JSON\"},\n",
    "        {\"role\": \"user\", \"content\": conversion_prompt},\n",
    "    ],\n",
    "    response_format={\n",
    "        \"type\": \"json_object\"\n",
    "    },\n",
    "    temperature=0.0,\n",
    "    max_tokens=10000\n",
    ")\n",
    "\n",
    "\n",
    "openai_tools_json = response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(json.dumps(json.loads(openai_tools_json), indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def handle_function_call(mcp_url, function_name, arguments):\n",
    "    \"\"\"\n",
    "    Obsługuje wywołanie funkcji na serwerze MCP.\n",
    "\n",
    "    Parametry:\n",
    "    mcp_url (str): URL serwera MCP.\n",
    "    function_name (str): Nazwa funkcji, którą należy wywołać.\n",
    "    arguments (dict): Argumenty przekazywane do funkcji.\n",
    "\n",
    "    Zwraca:\n",
    "    str: Wynik wywołania funkcji w postaci tekstu.\n",
    "    \"\"\"\n",
    "    print(f\"<function_call> Function: {function_name}, Arguments: {arguments}\")\n",
    "\n",
    "    # Nawiązanie połączenia SSE\n",
    "    async with sse_client(url=mcp_url) as (read, write):\n",
    "        # Utworzenie sesji klienta\n",
    "        async with ClientSession(read, write) as session:\n",
    "            # Inicjalizacja połączenia\n",
    "            await session.initialize()\n",
    "            \n",
    "            # Wywołanie funkcji na serwerze MCP\n",
    "            result = await session.call_tool(function_name, arguments=arguments)\n",
    "\n",
    "    return result.content[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await handle_function_call(mcp_url=mcp_url, function_name=\"add\", arguments={\"a\": \"2\", \"b\": \"3\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def execute_tool_calls(response, messages):\n",
    "    \"\"\"\n",
    "    Wykonuje funkcje zlecone przez model i aktualizuje historię wiadomości.\n",
    "\n",
    "    Parametry:\n",
    "    response (openai.types.chat.chat_completion.ChatCompletion): Odpowiedź od modelu zawierająca zlecenia funkcji.\n",
    "    messages (list): Historia wiadomości.\n",
    "\n",
    "    Zwraca:\n",
    "    list: Zaktualizowana historia wiadomości.\n",
    "    \"\"\"\n",
    "\n",
    "    # dodajemy odpowiedź, w tym ew. prośby o wywołanie funkcji, do historii wiadomości\n",
    "    messages.append(response.choices[0].message.model_dump())\n",
    "\n",
    "    for tool_call in response.choices[0].message.tool_calls:\n",
    "        # pobieramy nazwę funkcji i argumenty z odpowiedzi od modelu\n",
    "        name = tool_call.function.name\n",
    "        args = json.loads(tool_call.function.arguments)\n",
    "\n",
    "        # wywołanie funkcji na podstawie pobranej nazwy funkcji i argumentów\n",
    "        result = await handle_function_call(mcp_url, name, args)\n",
    "\n",
    "        # dodajemy wynik funkcji do historii wiadomości\n",
    "        messages.append({\n",
    "            \"role\": \"tool\",\n",
    "            \"tool_call_id\": tool_call.id,\n",
    "            \"content\": result\n",
    "        })\n",
    "\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wczytanie listy narzędzi z serwera MCP\n",
    "tools = json.loads(openai_tools_json)['tools']\n",
    "\n",
    "async def process_user_command(user_command):\n",
    "    \"\"\"\n",
    "    Przetwarza polecenie użytkownika z użyciem GPT-4o oraz funkcji odczytanych z serwera MCP.\n",
    "\n",
    "    Parametry:\n",
    "    user_command (str): Polecenie użytkownika.\n",
    "\n",
    "    Zwraca:\n",
    "    str: Odpowiedź na polecenie użytkownika.\n",
    "    \"\"\"\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"Jesteś pomocnym asystentem.\"},\n",
    "        {\"role\": \"user\", \"content\": user_command}\n",
    "    ]\n",
    "    \n",
    "    while True:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=messages,\n",
    "            tools=tools,\n",
    "            tool_choice=\"auto\",\n",
    "            temperature=0.0,\n",
    "            max_tokens=16000\n",
    "        )\n",
    "        \n",
    "        message = response.choices[0].message\n",
    "        \n",
    "        # Jeśli model żąda wykonania funkcji, wykonujemy je i aktualizujemy historię wiadomości\n",
    "        # (nastąpi kolejne wykonanie pętli zawierające request do modelu, w którym przekażemy wyniki wykonania funkcji)\n",
    "        if getattr(message, \"tool_calls\", None):\n",
    "            messages = await execute_tool_calls(response, messages)\n",
    "        else:\n",
    "            # Gdy nie ma już żądań funkcji, zwracamy ostateczną odpowiedź modelu. Ale najpierw uzupełniamy historię wiadomości\n",
    "            messages.append(response.choices[0].message.model_dump())\n",
    "\n",
    "            return message.content, messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result, messages = await process_user_command(\"Ile wynosi: 3+ln(3)*3?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pydantic AI\n",
    "\n",
    "Komunikacja z serwerem MCP (pobieranie listy funkcji, tłumaczenie ich opisu oraz wykonywanie funkcji) jest obsługiwana przez bibliotekę."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic_ai import Agent\n",
    "from pydantic_ai.models.openai import OpenAIModel\n",
    "from pydantic_ai.providers.openai import OpenAIProvider\n",
    "from pydantic_ai.mcp import MCPServerHTTP\n",
    "import os\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcp_url = \"http://mcp:8801/sse\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcp_server = MCPServerHTTP(url=mcp_url)\n",
    "\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "provider = OpenAIProvider(api_key=api_key)\n",
    "model = OpenAIModel('gpt-4o', provider=provider)\n",
    "\n",
    "agent = Agent(\n",
    "    model = model,\n",
    "    system_prompt='Bądź zwięzły, odpowiadaj jednym zdaniem.',\n",
    "    mcp_servers=[mcp_server]\n",
    ")\n",
    "\n",
    "# Albo Gemini jako alternatywa\n",
    "# agent = Agent(\n",
    "#     model = 'gemini-2.5-pro-exp-03-25',\n",
    "#     system_prompt='Użyj dostępnych narzędzi MCP aby obsłużyć zapytanie.',\n",
    "#     mcp_servers=[mcp_server]\n",
    "# )\n",
    "\n",
    "async with agent.run_mcp_servers():\n",
    "    result = agent.run_sync('Ile wynosi: 3+ln(3)*3?')\n",
    "\n",
    "print(result.output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open AI agents\n",
    "\n",
    "Podobnie jak w przypadku Pydantic AI, Open AI Agents automatyzuje obsługę komunikacji z serwerem MCP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents import Agent, Runner\n",
    "from agents.mcp import MCPServerSse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcp_server = MCPServerSse(params={\"url\": \"http://mcp:8801/sse\",})\n",
    "\n",
    "await mcp_server.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agent pobiera klucz ze zmiennej środowiskowej\n",
    "# ale można go ustawić bezpośrednio w kodzie:\n",
    "# import os\n",
    "# os.environ[\"OPENAI_API_KEY\"] = 'api_key'\n",
    "\n",
    "agent = Agent(\n",
    "    name=\"Agent MCP\",\n",
    "    model=\"gpt-4.5-preview\",\n",
    "    instructions=\"Użyj dostępnych narzędzi MCP aby obsłużyć zapytanie.\",\n",
    "    mcp_servers=[mcp_server]\n",
    ")\n",
    "\n",
    "result = await Runner.run(starting_agent=agent, input=\"Ile wynosi: 3+ln(3)*3?\")\n",
    "\n",
    "print(result.final_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I wersja z bezpośrednim przekazaniem klucza API "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents import OpenAIChatCompletionsModel\n",
    "from openai import AsyncOpenAI\n",
    "import os\n",
    "\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "client = AsyncOpenAI(api_key=api_key)\n",
    "model = OpenAIChatCompletionsModel(model=\"gpt-4o\", openai_client=client)\n",
    "\n",
    "agent = Agent(\n",
    "    name=\"Agent MCP\",\n",
    "    model=model,\n",
    "    instructions=\"Użyj dostępnych narzędzi MCP aby obsłużyć zapytanie.\",\n",
    "    mcp_servers=[mcp_server]\n",
    ")\n",
    "\n",
    "result = await Runner.run(starting_agent=agent, input=\"Ile wynosi: 3+ln(3)*3?\")\n",
    "\n",
    "print(result.final_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zasoby w MCP\n",
    "\n",
    "Oprócz narzędzi (funkcji), Model Context Protocol umożliwia również dostęp do zasobów. W przeciwieństwie do narzędzi, które wykonują jakieś obliczenia lub operacje, zasoby reprezentują dane statyczne lub dynamicznie generowane treści.\n",
    "\n",
    "Na naszym serwerze MCP zdefiniowane są dwa zasoby:\n",
    "1. `resource://math_symbols` - zwraca listę wszystkich dostępnych kluczy symboli matematycznych\n",
    "2. `resource://math_symbol/{symbol_name}` - zwraca konkretny symbol matematyczny dla podanej nazwy\n",
    "\n",
    "Przyjrzyjmy się, jak możemy odczytać te zasoby z serwera MCP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adres serwera MCP\n",
    "mcp_url = \"http://mcp:8801/sse\"\n",
    "\n",
    "# Aby korzysztać z lokalnego serwera MCP, uruchom go w terminalu wspisując:\n",
    "# > export MCP_PORT=1234\n",
    "# > python mcp_scripts/math_functions.py \n",
    "# i odkomentuj poniższą linię\n",
    "# mcp_url = \"http://localhost:1234/sse\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# funkcja pobierająca listę zasobów z serwera MCP\n",
    "async def mcp_list_resources(mcp_url):\n",
    "    \"\"\"\n",
    "    Pobiera listę dostępnych zasobów z serwera MCP.\n",
    "\n",
    "    Parametry:\n",
    "    mcp_url (str): URL serwera MCP.\n",
    "\n",
    "    Zwraca:\n",
    "    list: Lista dostępnych zasobów w formacie zwracanym przez serwer MCP.\n",
    "    \"\"\"\n",
    "    # Nawiązanie połączenia SSE\n",
    "    async with sse_client(url=mcp_url) as (read, write):\n",
    "        # Utworzenie sesji klienta\n",
    "        async with ClientSession(read, write) as session:\n",
    "            # Inicjalizacja połączenia\n",
    "            await session.initialize()\n",
    "\n",
    "            # Pobranie listy dostępnych zasobów\n",
    "            resources_response = await session.list_resources()\n",
    "\n",
    "            return resources_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pobranie listy dostępnych zasobów\n",
    "resources = await mcp_list_resources(mcp_url)\n",
    "resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jak widzimy, metoda `list_resources()` zwraca tylko statyczne zasoby (w naszym przypadku `resource://math_symbols`), ale nie wyświetla zasobów dynamicznych (z parametrami), takich jak `resource://math_symbol/{symbol_name}`.\n",
    "\n",
    "Jest to standardowe zachowanie protokołu MCP - zasoby dynamiczne nie są zwracane w listach zasobów, ponieważ wymagają podania parametrów do określenia konkretnego zasobu.\n",
    "\n",
    "Aby dowiedzieć się o dostępnych zasobach dynamicznych, należy zapoznać się z dokumentacją serwera MCP lub jego kodem źródłowym."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def mcp_get_resource(mcp_url, resource_uri):\n",
    "    \"\"\"\n",
    "    Pobiera zawartość zasobu z serwera MCP.\n",
    "\n",
    "    Parametry:\n",
    "    mcp_url (str): URL serwera MCP.\n",
    "    resource_uri (str): URI zasobu do pobrania.\n",
    "\n",
    "    Zwraca:\n",
    "    any: Zawartość zasobu.\n",
    "    \"\"\"\n",
    "    # Nawiązanie połączenia SSE\n",
    "    async with sse_client(url=mcp_url) as (read, write):\n",
    "        # Utworzenie sesji klienta\n",
    "        async with ClientSession(read, write) as session:\n",
    "            # Inicjalizacja połączenia\n",
    "            await session.initialize()\n",
    "\n",
    "            # Pobranie zawartości zasobu\n",
    "            resource_response = await session.read_resource(resource_uri)\n",
    "\n",
    "            return resource_response.contents[0].text\n",
    "\n",
    "# Odczytanie zasobu statycznego z listą symboli matematycznych\n",
    "math_symbols_text = await mcp_get_resource(mcp_url, \"resource://math_symbols\")\n",
    "math_symbols = json.loads(math_symbols_text)\n",
    "\n",
    "print(math_symbols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Odczytanie zasobu dynamicznego dla konkretnego symbolu matematycznego\n",
    "symbol_name = \"pi\"  # możesz zmienić na dowolny inny symbol z listy\n",
    "symbol = await mcp_get_resource(mcp_url, f\"resource://math_symbols/{symbol_name}\")\n",
    "\n",
    "print(f\"Symbol '{symbol_name}': {symbol}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Przykład odczytania kilku symboli matematycznych\n",
    "symbols_to_get = [\"pi\", \"integral\", \"square_root\", \"infinity\"]\n",
    "\n",
    "for symbol_name in symbols_to_get:\n",
    "    symbol = await mcp_get_resource(mcp_url, f\"resource://math_symbols/{symbol_name}\")\n",
    "    print(f\"Symbol '{symbol_name}': {symbol}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Podsumowanie pracy z zasobami MCP\n",
    "\n",
    "Zasoby MCP są użyteczne do udostępniania statycznych lub dynamicznie generowanych danych dla modeli językowych:\n",
    "\n",
    "1. **Zasoby statyczne** (np. `resource://math_symbols`) mogą zawierać listy, słowniki lub inne dane, które są zawsze takie same.\n",
    "\n",
    "2. **Zasoby dynamiczne** (np. `resource://math_symbol/{symbol_name}`) mogą generować dane na podstawie parametrów zawartych w URI.\n",
    "\n",
    "3. **Metoda `list_resources()`** zwraca tylko statyczne zasoby, nie pokazuje zasobów dynamicznych z parametrami.\n",
    "\n",
    "4. Aby odczytać zasób, używamy metody `get_resource(resource_uri)` z odpowiednim URI.\n",
    "\n",
    "Zasoby mogą być wykorzystywane przez modele językowe do pobierania danych referencyjnych, konfiguracji, lub innych informacji bez konieczności używania function callingu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
