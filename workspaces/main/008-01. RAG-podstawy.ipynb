{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a638b682",
   "metadata": {},
   "source": [
    "# Retrieval-Augmented Generation (RAG)\n",
    "\n",
    "## Wprowadzenie\n",
    "\n",
    "Retrieval-Augmented Generation (RAG) to podejście łączące systemy wyszukiwania informacji z generatywnymi modelami językowymi (LLM). Pozwala to na tworzenie odpowiedzi, które są zarówno precyzyjne (dzięki zewnętrznemu źródłu wiedzy) jak i naturalnie sformułowane (dzięki zdolnościom generatywnym LLM).\n",
    "\n",
    "### Dlaczego RAG jest istotny?\n",
    "\n",
    "1. **Aktualność informacji** - LLM-y są trenowane na danych historycznych i nie posiadają wiedzy o wydarzeniach po dacie odcięcia treningu\n",
    "2. **Redukcja konfabulacji** - Poprzez dostarczenie wiarygodnych źródeł, LLM ma mniejszą tendencję do wymyślania nieprawdziwych informacji\n",
    "3. **Weryfikowalność** - Odpowiedzi mogą zawierać referencje do źródeł, co zwiększa ich wiarygodność\n",
    "4. **Specjalistyczna wiedza** - Można podłączyć modele do specjalistycznych źródeł wiedzy bez konieczności ich dotrenowywania"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d66727",
   "metadata": {},
   "source": [
    "## Komponenty systemu RAG\n",
    "\n",
    "Typowy system RAG składa się z trzech głównych komponentów:\n",
    "\n",
    "1. **Retriever - System wyszukiwania** - Odpowiedzialny za znalezienie odpowiednich informacji w źródle wiedzy\n",
    "2. **Model generatywny** - Wykorzystuje znalezione informacje i zapytanie użytkownika do stworzenia odpowiedzi\n",
    "3. **Żródło wiedzy** - Źródło informacji, które może być przeszukiwane\n",
    "\n",
    "![RAG Architecture](assets/008-02.%20Architektura%20RAG.png)\n",
    "Źródło: https://www.clarifai.com/blog/what-is-rag-retrieval-augmented-generation\n",
    "\n",
    "### Przepływ informacji w RAG\n",
    "\n",
    "1. Użytkownik zadaje pytanie\n",
    "2. System wyszukiwania znajduje odpowiednie dokumenty/informacje związane z pytaniem\n",
    "3. Znalezione informacje wraz z oryginalnym pytaniem są przekazywane do modelu generatywnego\n",
    "4. Model generuje odpowiedź bazującą zarówno na pytaniu, jak i dostarczonych informacjach\n",
    "5. Odpowiedź jest zwracana użytkownikowi, często z referencjami do źródeł"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6897dc0",
   "metadata": {},
   "source": [
    "## Przykład prostego RAGa korzystającego z wyszukiwania informacji w Internecie\n",
    "\n",
    "Poniżej zaimplementujemy prosty, ale skuteczny system RAG, który będzie:\n",
    "- Wyszukiwał informacje w internecie przy pomocy DuckDuckGo\n",
    "- Wykorzystywał LLMa do generowania odpowiedzi na podstawie znalezionych informacji"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f46eb95",
   "metadata": {},
   "source": [
    "### Import bibliotek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef29510f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List, Dict, Any\n",
    "from openai import OpenAI\n",
    "import time\n",
    "from IPython.display import Markdown, display\n",
    "# Biblioteka do wyszukiwania w DuckDuckGo (bardziej otwarta konkurencja dla Google)\n",
    "from duckduckgo_search import DDGS\n",
    "from pydantic import BaseModel, Field"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fafb3db3",
   "metadata": {},
   "source": [
    "### Konfiguracja klienta OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812ee6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "client = OpenAI(api_key=api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9951ec3",
   "metadata": {},
   "source": [
    "### Implementacja komponentu wyszukiwania (Retriever)\n",
    "\n",
    "W naszym przypadku, jako źródło wiedzy użyjemy internetu, a konkretnie wyszukiwarki DuckDuckGo. Stworzymy funkcję, która będzie wyszukiwać informacje na podstawie zapytania użytkownika."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378eeaae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_information(query: str, max_results: int = 5) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Wyszukuje informacje związane z zapytaniem użytkownika za pomocą DuckDuckGo.\n",
    "    \n",
    "    Args:\n",
    "        query: Zapytanie użytkownika\n",
    "        max_results: Maksymalna liczba wyników do zwrócenia\n",
    "        \n",
    "    Returns:\n",
    "        Lista słowników zawierających znalezione informacje\n",
    "    \"\"\"\n",
    "    # Inicjalizacja wyszukiwarki DuckDuckGo\n",
    "    ddgs = DDGS()\n",
    "    \n",
    "    # Wykonanie wyszukiwania\n",
    "    results = list(ddgs.text(query, max_results=max_results))\n",
    "    \n",
    "    # Dodanie pola id zawierającego liczbę porządkową do każdego elementu\n",
    "    for i, result in enumerate(results, 1):\n",
    "        result['id'] = i\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d13ab9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieve_information(\"szef kuchni\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ba43b6",
   "metadata": {},
   "source": [
    "### Implementacja komponentu generatywnego (Generator)\n",
    "\n",
    "Teraz stworzymy funkcję, która wykorzysta model GPT-4o do wygenerowania odpowiedzi na podstawie pytania użytkownika i znalezionych informacji."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb283e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(query: str, retrieved_info: List[Dict[str, Any]]) -> str:\n",
    "    \"\"\"\n",
    "    Generuje odpowiedź na podstawie zapytania i znalezionych informacji.\n",
    "    \n",
    "    Args:\n",
    "        query: Zapytanie użytkownika\n",
    "        retrieved_info: Lista słowników zawierających znalezione informacje\n",
    "        \n",
    "    Returns:\n",
    "        Wygenerowana odpowiedź\n",
    "    \"\"\"\n",
    "\n",
    "    # Przygotowanie kontekstu z wyszukanych informacji\n",
    "    context = \"\\n\\n\".join([\n",
    "        f\"Tytuł: {result.get('title', 'Brak tytułu')}\\n\"\n",
    "        f\"Opis: {result.get('body', 'Brak opisu')}\\n\"\n",
    "        f\"Źródło: {result.get('href', 'Brak źródła')}\"\n",
    "        for result in retrieved_info\n",
    "    ])\n",
    "    \n",
    "    # Instrukcje dla modelu\n",
    "    system_prompt = f\"\"\"\n",
    "    Jesteś pomocnym asystentem, który odpowiada na pytania użytkownika w oparciu o dostarczone konteksty.\n",
    "    Twoim zadaniem jest:\n",
    "    1. Przeanalizować dostarczone informacje i wybrać te, które są najbardziej istotne dla pytania.\n",
    "    2. Sformułować zwięzłą, ale kompletną odpowiedź opartą na tych informacjach (i tylko na nich - nie uzupełniaj własną wiedzą). \n",
    "    3. Gdy informacje są sprzeczne lub niepewne, zaznaczyć to w odpowiedzi.\n",
    "    4. Dostosować czas odpowiedzi do aktualnej daty i godziny - jeśli mówisz o wydarzeniach z przeszłości, użyj czasu przeszłego, a jeśli o przyszłości - czasu przyszłego.\n",
    "    5. Podać źródła użytych informacji w formie odnośników [1], [2], itp. na końcu odpowiedzi. Numer powinien odpowiadać wartośći pola 'id' źródła w kontekście.\n",
    "    6. Jeśli dostarczone informacje nie pozwalają na odpowiedź, przyznaj to uczciwie.\n",
    "    7. Odpowiedź w tym samym języku, w którym zadano pytanie, Nnawet jeśli część źródeł jest w innym języku.\n",
    "    \n",
    "    Odpowiedź powinna być napisana w języku pytania, prostym do zrozumienia, i dobrze zorganizowana.\n",
    "\n",
    "    Dodatkowe informacje:\n",
    "    - Aktualna data i czas: {time.strftime('%Y-%m-%d %H:%M:%S')}. Uwzględnij tę informację tworząc odpowiedź.\n",
    "    \"\"\"\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    Pytanie:\n",
    "    {query}\n",
    "    ---\n",
    "    Kontekst:\n",
    "    {context}\n",
    "    \"\"\"\n",
    "\n",
    "    # Przygotowanie wiadomości dla API\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    \n",
    "    # Wywołanie API\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=messages,\n",
    "        temperature=0.2,  # Niska temperatura dla bardziej deterministycznych odpowiedzi\n",
    "        max_tokens=5000  # Limit długości odpowiedzi\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c25095f",
   "metadata": {},
   "source": [
    "### Łączenie komponentów w kompletny system RAG\n",
    "\n",
    "Teraz połączymy wszystkie komponenty, aby stworzyć kompletny system RAG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18024369",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_system_simple(query: str, max_results: int = 5):\n",
    "    \"\"\"\n",
    "    Kompletny system RAG, który wyszukuje informacje i generuje odpowiedź.\n",
    "    \n",
    "    Args:\n",
    "        query: Zapytanie użytkownika\n",
    "        max_results: Maksymalna liczba wyników wyszukiwania\n",
    "        \n",
    "    Returns:\n",
    "        Wygenerowana odpowiedź\n",
    "    \"\"\"\n",
    "    print(f\"Zapytanie: {query}\")\n",
    "    print(\"Wyszukiwanie informacji...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Wyszukiwanie informacji\n",
    "    retrieved_info = retrieve_information(query, max_results)\n",
    "    retrieval_time = time.time() - start_time\n",
    "    print(f\"Znaleziono {len(retrieved_info)} wyników w {retrieval_time:.2f} sekund.\")\n",
    "    \n",
    "    print(\"Generowanie odpowiedzi...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Generowanie odpowiedzi\n",
    "    answer = generate_answer(query, retrieved_info)\n",
    "    generation_time = time.time() - start_time\n",
    "    print(f\"Odpowiedź wygenerowana w {generation_time:.2f} sekund.\")\n",
    "    \n",
    "    # Informacje o źródłach\n",
    "    print(\"\\nŹródła informacji:\")\n",
    "    for i, result in enumerate(retrieved_info, 1):\n",
    "        print(f\"[{result.get('id')}] {result.get('title')}: {result.get('href')}\")\n",
    "    \n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da2a9be",
   "metadata": {},
   "source": [
    "### Funkcja pomocnicza do wyświetlania wyników"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa4d3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_answer(answer):\n",
    "    \"\"\"\n",
    "    Wyświetla odpowiedź w ładnym formacie Markdown.\n",
    "    \n",
    "    Args:\n",
    "        answer: Wygenerowana odpowiedź\n",
    "    \"\"\"\n",
    "    display(Markdown(\"---\\n## Odpowiedź:\\n\" + answer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08cebe9a",
   "metadata": {},
   "source": [
    "### Przykładowe wywołania\n",
    "\n",
    "Przetestujmy, czy to działa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab80a427",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Przykład 1: Zapytanie o aktualne wydarzenie\n",
    "query1 = \"Kiedy i gdzie odbyły się Igrzyska Olimpijskie 2024?\"\n",
    "answer1 = rag_system_simple(query1, max_results=10)\n",
    "display_answer(answer1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13cb24b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Przykład 2: Zapytanie techniczne\n",
    "query2 = \"Który model od Open AI jest najnowszy? Czym się różni od wcześniejszych modeli?\"\n",
    "answer2 = rag_system_simple(query2, max_results=10)\n",
    "display_answer(answer2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771a5301",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Przykład 3: Zapytanie o osobę\n",
    "query3 = \"Kto obecnie jest premierem Polski i jakie ma doświadczenie polityczne?\"\n",
    "answer3 = rag_system_simple(query3, max_results=10)\n",
    "display_answer(answer3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf11e1c",
   "metadata": {},
   "source": [
    "## Trochę bardziej zaawansowany RAG\n",
    "\n",
    "W bardziej zaawansowanej wersji RAG, zamiast używać pojedynczego zapytania wyszukiwania, i to będącego pytaniem użytkownika, możemy wygenerować kilka różnych zapytań, które lepiej uchwycą kontekst i intencję pytania użytkownika. To podejście, znane jako Multi-Query RAG, pomaga:\n",
    "\n",
    "1. **Uzyskać szerszy kontekst** - Różne warianty zapytań mogą wydobyć informacje, które nie byłyby dostępne przy pojedynczym zapytaniu\n",
    "2. **Lepiej radzić sobie z niejednoznacznościami** - Generując zapytania uwzględniające różne interpretacje pytania użytkownika\n",
    "3. **Zwiększyć prawdopodobieństwo znalezienia odpowiednich informacji** - Więcej zapytań = większa szansa na trafienie w źródła zawierające potrzebne dane\n",
    "\n",
    "Poniżej zaimplementujemy system RAG z generowaniem wielu zapytań wyszukiwania, używając modelu GPT-4o do stworzenia optymalnych haseł wyszukiwania na podstawie pierwotnego pytania użytkownika."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14476bc",
   "metadata": {},
   "source": [
    "### Funkcja tworząca zapytania dla polecenia użytkownika"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9ece15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_search_queries(question: str, num_queries: int = 3) -> list[str]:\n",
    "    \"\"\"\n",
    "    Generuje zoptymalizowane zapytania wyszukiwania dla DuckDuckGo na podstawie pytania użytkownika.\n",
    "    \n",
    "    Args:\n",
    "        question: Pytanie lub prośba użytkownika\n",
    "        num_queries: Liczba zapytań wyszukiwania do wygenerowania (domyślnie 3)\n",
    "        \n",
    "    Returns:\n",
    "        Lista 'num_queries' terminów wyszukiwania\n",
    "    \"\"\"\n",
    "    system_prompt = f\"\"\"\n",
    "    Jesteś ekspertem w tworzeniu efektywnych zapytań wyszukiwania.\n",
    "    Twoim zadaniem jest wygenerowanie zapytań do wyszukiwarki DuckDuckGo, \n",
    "    które pozwolą zebrać najbardziej przydatne informacje do odpowiedzi na pytanie użytkownika.\n",
    "    \n",
    "    Zapytania powinny:\n",
    "    1. Być zwięzłe i konkretne\n",
    "    2. Zawierać kluczowe słowa i frazy\n",
    "    3. Obejmować różne aspekty pytania lub tematu\n",
    "    4. Być sformułowane w taki sposób, aby zmaksymalizować trafność wyników wyszukiwania\n",
    "    5. Pozwolić na wyszukiwanie informacji w różnych źródłach, nie tylko w encyklopediach czy artykułach naukowych\n",
    "    6. Pozwolić na wyszukanie informacji w języku pytania użytkownika, oraz w języku angielskim\n",
    "    \n",
    "    Dodatkowe informacje:\n",
    "    - Aktualna data i czas: {time.strftime('%Y-%m-%d %H:%M:%S')}. Uwzględnij tę informację tworząc odpowiedź.\n",
    "    \"\"\"\n",
    "    \n",
    "    user_prompt = f\"\"\"\n",
    "    Stwórz dokładnie {num_queries} zapytań wyszukiwania do DuckDuckGo dla następującego pytania:\n",
    "    ---\n",
    "    {question}\n",
    "    \"\"\"\n",
    "    \n",
    "    class SearchQueries(BaseModel):\n",
    "        \"\"\"Klasa pydanticowa dla zapytań wyszukiwania wygenerowanych na podstawie pytania użytkownika.\"\"\"\n",
    "        queries: list[str] = Field(description=f\"Lista dokładnie {num_queries} zapytań wyszukiwania do użycia z DuckDuckGo\")\n",
    "    \n",
    "    # Wywołanie modelu GPT-4o\n",
    "    response = client.beta.chat.completions.parse(\n",
    "        model=\"gpt-4o\",\n",
    "        response_format=SearchQueries,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=0.2  # Niska temperatura dla bardziej deterministycznych odpowiedzi\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.parsed.queries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e762aefa",
   "metadata": {},
   "source": [
    "Zobaczmy czy to działa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889c3269",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_search_queries(\"Jakie są najnowsze osiągnięcia w dziedzinie sztucznej inteligencji?\", num_queries=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d981ac83",
   "metadata": {},
   "source": [
    "### RAG uwzględniający wyszukiwania wieloma hasłami"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bedd84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_system_multiquery(query: str, max_results_per_query: int = 3, num_queries: int = 3):\n",
    "    \"\"\"\n",
    "    Zaawansowany system RAG wykorzystujący wielokrotne zapytania wyszukiwania.\n",
    "    \n",
    "    Args:\n",
    "        query: Zapytanie użytkownika\n",
    "        max_results_per_query: Maksymalna liczba wyników wyszukiwania dla każdego z wygenerowanych zapytań\n",
    "        num_queries: Liczba zapytań wyszukiwania do wygenerowania (domyślnie 3)\n",
    "        \n",
    "    Returns:\n",
    "        Wygenerowana odpowiedź\n",
    "    \"\"\"\n",
    "    print(f\"Zapytanie użytkownika: {query}\")\n",
    "    print(f\"Generowanie {num_queries} optymalnych zapytań wyszukiwania...\")\n",
    "    \n",
    "    # Generowanie zapytań do wyszukiwania\n",
    "    search_queries = generate_search_queries(question=query, num_queries=num_queries)\n",
    "    print(f\"Wygenerowano {len(search_queries)} zapytań wyszukiwania:\")\n",
    "    for idx, search_query in enumerate(search_queries, 1):\n",
    "        print(f\"  {idx}. {search_query}\")\n",
    "    \n",
    "    # Wyszukiwanie informacji dla każdego zapytania\n",
    "    print(\"\\nWyszukiwanie informacji...\")\n",
    "    start_time = time.time()\n",
    "    all_results = []\n",
    "    \n",
    "    # Słownik do śledzenia liczby wyników dla każdego zapytania\n",
    "    results_per_query = {}\n",
    "    \n",
    "    for i, search_query in enumerate(search_queries, 1):\n",
    "        results = retrieve_information(search_query, max_results=max_results_per_query)\n",
    "        # Dodanie informacji o zapytaniu wyszukiwania do każdego wyniku\n",
    "        for result in results:\n",
    "            result['search_query'] = search_query\n",
    "        results_per_query[search_query] = len(results)        \n",
    "        all_results.extend(results)\n",
    "    \n",
    "    retrieval_time = time.time() - start_time\n",
    "    print(f\"Znaleziono łącznie {len(all_results)} wyników w {retrieval_time:.2f} sekund.\")\n",
    "    \n",
    "    # Wypisanie liczby wyników dla każdego zapytania\n",
    "    print(\"\\nLiczba wyników dla poszczególnych zapytań:\")\n",
    "    for i, (query, count) in enumerate(results_per_query.items(), 1):\n",
    "        print(f\"  [{i}] {query}: {count} wyników\")\n",
    "    \n",
    "    # Usuwanie duplikatów na podstawie URL i treści\n",
    "    unique_content = set()\n",
    "    unique_results = []\n",
    "    \n",
    "    for result in all_results:\n",
    "        # Tworzenie unikalnego identyfikatora na podstawie URL i fragmentu treści\n",
    "        # Używamy tylko pierwszych 100 znaków treści, aby uniknąć nadmiernego zużycia pamięci\n",
    "        content_id = (result['href'], result.get('body', '')[:100])\n",
    "        \n",
    "        if content_id not in unique_content:\n",
    "            unique_content.add(content_id)\n",
    "            unique_results.append(result)\n",
    "    \n",
    "    # Reset ID dla unikalnych wyników\n",
    "    for i, result in enumerate(unique_results, 1):\n",
    "        result['id'] = i\n",
    "    \n",
    "    print(f\"Po usunięciu duplikatów pozostało {len(unique_results)} unikalnych wyników.\")\n",
    "    \n",
    "    # Generowanie odpowiedzi\n",
    "    print(\"\\nGenerowanie odpowiedzi na podstawie znalezionych informacji...\")\n",
    "    start_time = time.time()\n",
    "    answer = generate_answer(query, unique_results)\n",
    "    generation_time = time.time() - start_time\n",
    "    print(f\"Odpowiedź wygenerowana w {generation_time:.2f} sekund.\")\n",
    "    \n",
    "    # Informacje o źródłach\n",
    "    print(\"\\nŹródła informacji:\")\n",
    "    for result in unique_results:\n",
    "        print(f\"[{result['id']}] {result['title']}: {result['href']}\")\n",
    "        print(f\"    Znaleziono przez zapytanie: {result.get('search_query', 'nieznane zapytanie')}\")\n",
    "    \n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e080476",
   "metadata": {},
   "source": [
    "### Przykłady wywołań"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a32d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Przykład 1: Zapytanie o aktualne wydarzenie\n",
    "query1 = \"Kiedy i gdzie odbyły się Igrzyska Olimpijskie 2024? O czym mówiły media?\"\n",
    "answer1 = rag_system_multiquery(query1, max_results_per_query=5, num_queries=5)\n",
    "display_answer(answer1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b37c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Przykład 2: Zapytanie techniczne\n",
    "query2 = \"Który model od Open AI jest najnowszy? Czym się różni od wcześniejszych modeli?\"\n",
    "answer2 = rag_system_multiquery(query2, max_results_per_query=5, num_queries=5)\n",
    "display_answer(answer2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cacebfed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Przykład 3: Zapytanie o osobę\n",
    "query3 = \"Kto obecnie jest premierem Polski i jakie ma doświadczenie polityczne?\"\n",
    "answer3 = rag_system_multiquery(query3, max_results_per_query=5, num_queries=5)\n",
    "display_answer(answer3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce22dc9",
   "metadata": {},
   "source": [
    "## RAG wykorzystujący indeks wektorowy\n",
    "\n",
    "W poprzednich przykładach używaliśmy wyszukiwania w internecie jako źródła wiedzy. Alternatywnym, często bardziej efektywnym podejściem jest wykorzystanie bazy wektorowej do przechowywania i wyszukiwania dokumentów.\n",
    "\n",
    "Podejście oparte o indeksy / bazy wektorowe ma kilka zalet:\n",
    "\n",
    "1. **Wyszukiwanie semantyczne** - Zamiast dopasowywać słowa kluczowe, wyszukujemy dokumenty o podobnym znaczeniu\n",
    "2. **Szybkość** - Lokalne przechowywanie dokumentów umożliwia szybkie wyszukiwanie bez opóźnień związanych z zapytaniami do zewnętrznych API\n",
    "3. **Kontrola nad danymi** - Mamy pełną kontrolę nad tym, jakie dokumenty są w bazie wiedzy\n",
    "4. **Prywatność** - Dokumenty i zapytania nie muszą opuszczać naszego środowiska\n",
    "\n",
    "Poniżej zaimplementujemy prosty system RAG wykorzystujący bazę wektorową FAISS i model sentence-transformers do tworzenia wektorów (embeddings)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dedc9f67",
   "metadata": {},
   "source": [
    "### Import bibliotek dla wersji z indeksem wektorowym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3355607d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import faiss  # Biblioteka do efektywnego wyszukiwania wektorów\n",
    "from sentence_transformers import SentenceTransformer  # Do tworzenia wektorów tekstu\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49e6ec8",
   "metadata": {},
   "source": [
    "### Przygotowanie przykładowych dokumentów\n",
    "\n",
    "W prawdziwym systemie te dokumenty mogłyby pochodzić z różnych źródeł, takich jak bazy wiedzy, strony internetowe, PDF-y, itp.\n",
    "Na potrzeby tego przykładu stworzymy prosty zbiór dokumentów."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edcc889f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Przykładowe dokumenty dla naszej bazy wiedzy\n",
    "\n",
    "documents = [\n",
    "    # Dokumenty z kategorii sztuczna inteligencja\n",
    "    \"Sztuczna inteligencja (AI) to dziedzina informatyki zajmująca się tworzeniem systemów zdolnych do wykonywania zadań wymagających inteligencji ludzkiej.\",\n",
    "    \"Uczenie maszynowe to podzbiór sztucznej inteligencji, który umożliwia systemom komputerowym uczenie się z danych bez wyraźnego programowania.\",\n",
    "    \"Deep learning to technika uczenia maszynowego wykorzystująca sieci neuronowe z wieloma warstwami (głębokie sieci neuronowe) do analizy różnych czynników w danych.\",\n",
    "    \"Natural Language Processing (NLP) to dziedzina AI zajmująca się interakcją między komputerami a językiem ludzkim.\",\n",
    "    \"Computer Vision to dziedzina AI, która trenuje komputery do interpretowania i rozumienia świata wizualnego.\",\n",
    "    \"Reinforcement learning to obszar uczenia maszynowego dotyczący podejmowania decyzji przez agenta w środowisku w celu maksymalizacji skumulowanej nagrody.\",\n",
    "    \"Transformer to architektura sieci neuronowych oparta na mechanizmie uwagi, który poprawia wydajność w zadaniach przetwarzania języka naturalnego.\",\n",
    "    \"Model GPT-4o, wydany przez OpenAI w 2024 roku, to multimodalny model AI, który rozumie tekst i obrazy.\",\n",
    "    \"Systemy rekomendacyjne to aplikacje AI, które przewidują preferencje użytkowników i sugerują odpowiednie produkty lub treści.\",\n",
    "    \"Etyka AI dotyczy etycznych implikacji i wyzwań związanych z rozwojem i wdrażaniem systemów sztucznej inteligencji.\",\n",
    "    \"OpenAI to firma badawcza zajmująca się sztuczną inteligencją, znana z modeli takich jak GPT-3, GPT-4, DALL-E i Sora.\",\n",
    "    \"Sora to model AI od OpenAI zaprezentowany w 2024 roku, który potrafi generować realistyczne filmy wideo na podstawie instrukcji tekstowych.\",\n",
    "    \"Anthropic to firma AI założona przez byłych pracowników OpenAI, która stworzyła asystenta Claude.\",\n",
    "    \"Claude to duży model językowy (LLM) od Anthropic, konkurencyjny wobec modeli GPT, znany z funkcji Constitutional AI mających zapewnić bezpieczeństwo.\",\n",
    "    \"Gemini to model multimodalny od Google DeepMind, wcześniej znany jako Bard, wprowadzony jako konkurent ChatGPT.\",\n",
    "    \"Meta (wcześniej Facebook) rozwija własne modele AI, w tym Llama, który został udostępniony jako open source dla społeczności badawczej.\",\n",
    "    \"Hallucynacje AI to zjawisko, w którym modele generatywne tworzą nieprawdziwe informacje, które wydają się wiarygodne, ale nie mają potwierdzenia w faktach.\",\n",
    "    \"RAG (Retrieval-Augmented Generation) to metoda łącząca wyszukiwanie informacji z generatywnymi modelami językowymi w celu poprawy dokładności odpowiedzi.\",\n",
    "    \"Modele foundation to duże modele AI trenowane na ogromnych zbiorach danych, które mogą być dostosowywane do różnych zadań poprzez fine-tuning.\",\n",
    "    \"Fine-tuning to proces dostosowywania wstępnie wytrenowanego modelu AI do konkretnego zadania lub domeny poprzez dodatkowe szkolenie na specyficznych danych.\",\n",
    "    \"Prompt engineering to sztuka konstruowania efektywnych poleceń dla modeli AI w celu uzyskania pożądanych wyników.\",\n",
    "    \"AGI (Artificial General Intelligence) odnosi się do hipotetycznej sztucznej inteligencji, która posiada zdolność rozumienia, uczenia się i wykonywania dowolnego zadania intelektualnego, które może wykonać człowiek.\",\n",
    "    \"Transfer learning to technika uczenia maszynowego, w której model opracowany dla jednego zadania jest ponownie wykorzystywany jako punkt wyjścia dla modelu w drugim zadaniu.\",\n",
    "    \"Embeddingi to reprezentacje wektorowe słów, zdań lub innych danych, które wychwytują znaczenie semantyczne i są używane w wielu zastosowaniach AI.\",\n",
    "    \"Attention mechanism to kluczowy komponent architektur transformerowych, który pozwala modelom nadawać różne wagi różnym częściom danych wejściowych.\",\n",
    "    \"GPT (Generative Pre-trained Transformer) to rodzina modeli językowych opracowanych przez OpenAI, które wykorzystują architektury transformerowe do generowania tekstu.\",\n",
    "    \"BERT (Bidirectional Encoder Representations from Transformers) to model językowy opracowany przez Google, który reprezentuje kontekst słowa w obu kierunkach.\",\n",
    "    \"Zero-shot learning to zdolność modelu AI do wykonywania zadań bez wcześniejszego trenowania na przykładach tego konkretnego zadania.\",\n",
    "    \"Federated learning to technika uczenia maszynowego, w której model jest trenowany na wielu urządzeniach lub serwerach bez wymiany danych, chroniąc prywatność.\",\n",
    "    \"Explainable AI (XAI) to podejście do sztucznej inteligencji, które koncentruje się na tworzeniu modeli, których działania mogą być zrozumiane przez ludzi.\",\n",
    "    \n",
    "    # Dokumenty z kategorii cyberbezpieczeństwo\n",
    "    \"Cyberbezpieczeństwo to praktyka ochrony systemów, sieci i programów przed cyfrowymi atakami mającymi na celu dostęp, zmianę lub zniszczenie wrażliwych informacji.\",\n",
    "    \"Malware to złośliwe oprogramowanie, które celowo wyrządza szkody w komputerach, serwerach lub sieciach komputerowych, obejmujące wirusy, trojany i ransomware.\",\n",
    "    \"Phishing to cyberatak, w którym atakujący maskują się jako zaufana jednostka, aby nakłonić ofiary do ujawnienia poufnych danych, takich jak hasła czy dane karty kredytowej.\",\n",
    "    \"Firewall to system bezpieczeństwa sieciowego, który monitoruje i kontroluje przychodzący i wychodzący ruch sieciowy zgodnie z określonymi zasadami bezpieczeństwa.\",\n",
    "    \"VPN (Virtual Private Network) to technologia, która tworzy bezpieczne, szyfrowane połączenie przez mniej bezpieczną sieć, taką jak internet.\",\n",
    "    \"Szyfrowanie to proces kodowania informacji w taki sposób, aby tylko upoważnione osoby miały do nich dostęp za pomocą klucza szyfrowania.\",\n",
    "    \"Zero-day exploit to atak wykorzystujący nieznane wcześniej luki w oprogramowaniu, o których programiści nie wiedzą i nie mają dla nich łatek.\",\n",
    "    \"SIEM (Security Information and Event Management) to narzędzie zapewniające analizę alertów bezpieczeństwa generowanych przez aplikacje i urządzenia sieciowe.\",\n",
    "    \"Dwuskładnikowe uwierzytelnianie (2FA) to metoda bezpieczeństwa, która wymaga dwóch niezależnych form identyfikacji do uzyskania dostępu do zasobów.\",\n",
    "    \"SOC (Security Operations Center) to zespół ekspertów ds. bezpieczeństwa, którzy monitorują i analizują środowisko bezpieczeństwa organizacji.\",\n",
    "    \"CSRF (Cross-Site Request Forgery) to atak, który wymusza na zalogowanym użytkowniku wykonanie niechcianych działań na stronie internetowej.\",\n",
    "    \"XSS (Cross-Site Scripting) to luka w zabezpieczeniach, która umożliwia atakującym wstrzykiwanie złośliwych skryptów do stron internetowych oglądanych przez innych użytkowników.\",\n",
    "    \"SQL injection to technika ataku, w której złośliwy kod SQL jest wprowadzany do pól wejściowych formularza w celu uzyskania dostępu do bazy danych.\",\n",
    "    \"Ransomware to rodzaj złośliwego oprogramowania, które blokuje dostęp do plików lub systemów komputerowych i żąda okupu za ich odblokowanie.\",\n",
    "    \"DDoS (Distributed Denial of Service) to atak, w którym wiele systemów przytłacza cel (jak serwer, strona internetowa lub inna sieć) generując ruch internetowy.\",\n",
    "    \"Pentesting (testowanie penetracyjne) to symulowany atak na system komputerowy w celu oceny bezpieczeństwa systemu i identyfikacji luk.\",\n",
    "    \"Biometria to metoda uwierzytelniania oparta na unikalnych cechach fizycznych, takich jak odciski palców, rozpoznawanie twarzy czy skanowanie tęczówki.\",\n",
    "    \"OSINT (Open-Source Intelligence) to zbieranie informacji z publicznie dostępnych źródeł do celów analizy bezpieczeństwa.\",\n",
    "    \"APT (Advanced Persistent Threat) to długotrwały, ukierunkowany cyberatak, w którym atakujący zyskuje nieautoryzowany dostęp do sieci i pozostaje niewykryty przez dłuższy czas.\",\n",
    "    \"Blockchain to technologia rozproszonej księgi, która zwiększa bezpieczeństwo poprzez przechowywanie danych w blokach połączonych kryptograficznie.\",\n",
    "    \n",
    "    # Dokumenty z kategorii frontend\n",
    "    \"HTML (HyperText Markup Language) to standardowy język znaczników używany do tworzenia stron internetowych i aplikacji webowych.\",\n",
    "    \"CSS (Cascading Style Sheets) to język arkuszy stylów używany do opisywania prezentacji dokumentu napisanego w języku znaczników, takim jak HTML.\",\n",
    "    \"JavaScript to wysokopoziomowy, dynamicznie typowany język programowania, który jest jednym z podstawowych technologii World Wide Web.\",\n",
    "    \"React to biblioteka JavaScript stworzona przez Facebooka do budowania interfejsów użytkownika, szczególnie aplikacji jednostronicowych.\",\n",
    "    \"Angular to platforma TypeScript i framework do budowania aplikacji webowych i mobilnych, rozwijana przez Google.\",\n",
    "    \"Vue.js to progresywny framework JavaScript do budowania interfejsów użytkownika, zaprojektowany by być przyswajany stopniowo.\",\n",
    "    \"TypeScript to silnie typowany nadzbiór JavaScript, który kompiluje się do czystego JavaScript, opracowany przez Microsoft.\",\n",
    "    \"Webpack to narzędzie do pakowania modułów dla nowoczesnych aplikacji JavaScript, przetwarzające aplikację i budujące graf zależności.\",\n",
    "    \"Responsive Web Design to podejście do projektowania stron internetowych, które sprawia, że strony dobrze wyświetlają się na urządzeniach o różnych rozmiarach i orientacjach.\",\n",
    "    \"PWA (Progressive Web App) to typ aplikacji dostarczanej za pośrednictwem sieci, ale oferującej funkcje podobne do natywnych aplikacji mobilnych.\",\n",
    "    \"Babel to kompilator JavaScript, który przekształca nowoczesny kod JavaScript w wersję kompatybilną wstecz dla starszych przeglądarek.\",\n",
    "    \"Redux to kontener stanu przewidywalnego dla aplikacji JavaScript, często używany z Reactem do zarządzania stanem aplikacji.\",\n",
    "    \"SASS (Syntactically Awesome Style Sheets) to preprocesor CSS, który rozszerza możliwości CSS o zmienne, zagnieżdżone reguły i inne funkcje.\",\n",
    "    \"Tailwind CSS to framework CSS oparty na klasach narzędziowych, który promuje szybkie tworzenie i dostosowywanie interfejsów bez opuszczania HTML.\",\n",
    "    \"Material Design to język projektowania opracowany przez Google, który syntetyzuje klasyczne zasady dobrego designu z innowacjami technologii i nauki.\",\n",
    "    \"SVG (Scalable Vector Graphics) to format grafiki wektorowej oparty na XML, używany do tworzenia dwuwymiarowych grafik z obsługą interaktywności i animacji.\",\n",
    "    \"AJAX (Asynchronous JavaScript and XML) to zestaw technik programowania wykorzystywanych do tworzenia asynchronicznych aplikacji internetowych.\",\n",
    "    \"SPA (Single Page Application) to aplikacja internetowa lub strona, która dynamicznie aktualizuje bieżącą stronę zamiast ładować całe nowe strony z serwera.\",\n",
    "    \"SEO (Search Engine Optimization) to proces zwiększania jakości i ilości ruchu na stronie internetowej z organicznych wyników wyszukiwania.\",\n",
    "    \"WebSocket to protokół komunikacyjny zapewniający dwukierunkowy kanał komunikacji przez pojedyncze połączenie TCP.\",\n",
    "    \n",
    "    # Dokumenty z kategorii języki backendu\n",
    "    \"Node.js to środowisko uruchomieniowe JavaScript, które pozwala na wykonywanie kodu JavaScript poza przeglądarką, używane głównie do budowania szybkich aplikacji sieciowych.\",\n",
    "    \"Python to interpretowany, wysokopoziomowy i ogólnego przeznaczenia język programowania, znany z czytelnej składni i szerokiego zastosowania od web developmentu po naukę o danych i AI.\",\n",
    "    \"Java to wieloplatformowy, obiektowy język programowania, szeroko stosowany do tworzenia aplikacji enterprise i systemów korporacyjnych.\",\n",
    "    \"C# to obiektowy język programowania opracowany przez Microsoft, używany głównie do tworzenia aplikacji Windows i aplikacji webowych z .NET Framework.\",\n",
    "    \"PHP to skryptowy język programowania zaprojektowany specjalnie do tworzenia stron internetowych i aplikacji webowych.\",\n",
    "    \"Ruby to dynamiczny, open-source język programowania z naciskiem na prostotę i produktywność, często używany z frameworkiem Rails do tworzenia aplikacji webowych.\",\n",
    "    \"Go (Golang) to język programowania stworzony przez Google, zaprojektowany do budowania prostego, niezawodnego i wydajnego oprogramowania.\",\n",
    "    \"Rust to wieloparadygmatowy język programowania systemowego skupiony na bezpieczeństwie pamięci bez użycia garbage collectora.\",\n",
    "    \"SQL (Structured Query Language) to standardowy język używany do komunikacji z relacyjnymi bazami danych, umożliwiający manipulację danymi i ich zapytania.\",\n",
    "    \"MongoDB to baza danych NoSQL typu document-store, która przechowuje dane w elastycznych dokumentach podobnych do JSON, zamiast w tabelach relacyjnych.\",\n",
    "    \"Redis to sklep struktur danych typu open source (in-memory), używany jako baza danych, pamięć podręczna i broker wiadomości.\",\n",
    "    \"GraphQL to język zapytań API i środowisko uruchomieniowe do realizacji tych zapytań z istniejącymi danymi, oferujący bardziej efektywne i elastyczne podejście niż REST.\",\n",
    "    \"Django to wysokopoziomowy framework internetowy w Pythonie, który zachęca do szybkiego rozwoju i czystego, pragmatycznego projektowania.\",\n",
    "    \"Express.js to minimalistyczny i elastyczny framework webowy dla Node.js, zapewniający zestaw funkcji do tworzenia aplikacji webowych i mobilnych.\",\n",
    "    \"Spring Boot to framework oparty na Javie, który upraszcza proces tworzenia aplikacji opartych na Spring, minimalizując konfigurację.\",\n",
    "    \"Laravel to framework aplikacji webowych z elegancką składnią dla PHP, ułatwiający typowe zadania programistyczne.\",\n",
    "    \"Ruby on Rails to framework aplikacji webowych napisany w Ruby, który następuje zasadzie 'convention over configuration' (konwencja przed konfiguracją).\",\n",
    "    \"ASP.NET Core to darmowy, open-source, cross-platform framework do budowania aplikacji internetowych na platformę .NET.\",\n",
    "    \"RESTful API to interfejs programistyczny aplikacji (API) zgodny z ograniczeniami architektury REST i umożliwiający interakcję z usługami webowymi.\",\n",
    "    \"Microservices to architektoniczny styl, w którym złożone aplikacje są podzielone na małe, niezależne usługi, które komunikują się przez sieć.\"\n",
    "]\n",
    "\n",
    "# Dodanie metadanych do dokumentów\n",
    "document_metadata = []\n",
    "for i, doc in enumerate(documents):\n",
    "    category = \"\"\n",
    "    if i < 30:\n",
    "        category = \"Sztuczna inteligencja\"\n",
    "    elif i < 50:\n",
    "        category = \"Cyberbezpieczeństwo\"\n",
    "    elif i < 70:\n",
    "        category = \"Frontend\"\n",
    "    else:\n",
    "        category = \"Backend\"\n",
    "        \n",
    "    document_metadata.append({\n",
    "        \"id\": i,\n",
    "        \"content\": doc,\n",
    "        \"source\": f\"Dokument {i}\",\n",
    "        \"category\": category\n",
    "    })\n",
    "\n",
    "documents = document_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60cd4b8",
   "metadata": {},
   "source": [
    "### Implementacja systemu wektorowego RAG\n",
    "\n",
    "Teraz zaimplementujemy poszczególne komponenty systemu RAG opartego o bazę wektorową jako osobne funkcje."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d590d369",
   "metadata": {},
   "source": [
    "#### Wyznaczenie embeddingów dla każdego dokumentu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644572b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicjalizacja modelu do tworzenia embeddingów\n",
    "embedding_model = SentenceTransformer(\"paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "\n",
    "# Wyodrębniamy same treści dokumentów\n",
    "texts = [doc[\"content\"] for doc in documents]\n",
    "\n",
    "# Generujemy wektory dla każdego dokumentu\n",
    "embeddings = embedding_model.encode(texts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cbc1172",
   "metadata": {},
   "source": [
    "#### Inicjalizacja indeksu wektorowego (i dodanie do niego embeddingów domkumetnów)\n",
    "\n",
    "Nie do końca moemy mówić tutaj o bazie wektorowej, bo FAISS nie przechowuje treści dokumentów. Jedynie pozwala nam efektywnie wyszukiwać indeksy najbardziej podobnych wektorów."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa25e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wymiary wektorów\n",
    "dimension = embeddings.shape[1]\n",
    "\n",
    "# Inicjalizacja indeksu FAISS\n",
    "index = faiss.IndexFlatIP(dimension)\n",
    "faiss.normalize_L2(embeddings)\n",
    "\n",
    "# Dodanie wektorów do indeksu\n",
    "index.add(np.array(embeddings).astype('float32'))\n",
    "\n",
    "print(f\"Utworzono bazę wektorową z {len(documents)} dokumentami o wymiarze {dimension}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0585f84",
   "metadata": {},
   "source": [
    "#### Wyszukiwanie dokumentów w indeksie wektorowym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba2b3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_documents_vector(query, documents, embedding_model, index, top_k=3):\n",
    "    \"\"\"\n",
    "    Wyszukuje najbardziej podobne dokumenty do zapytania.\n",
    "    \n",
    "    Args:\n",
    "        query: Zapytanie użytkownika\n",
    "        documents: Lista dokumentów z metadanymi\n",
    "        embedding_model: Model do generowania wektorów tekstu\n",
    "        index: Indeks FAISS bazy wektorowej\n",
    "        top_k: Liczba najbardziej podobnych dokumentów do zwrócenia\n",
    "        \n",
    "    Returns:\n",
    "        Lista słowników zawierających najbardziej podobne dokumenty\n",
    "    \"\"\"\n",
    "    # Generowanie wektora dla zapytania\n",
    "    query_embedding = embedding_model.encode([query])\n",
    "    faiss.normalize_L2(query_embedding)\n",
    "    \n",
    "    # Wyszukanie najbliższych wektorów\n",
    "    similarities, indices = index.search(np.array(query_embedding).astype('float32'), top_k)\n",
    "    \n",
    "    # Pobranie odpowiadających dokumentów\n",
    "    retrieved_docs = []\n",
    "    for i, idx in enumerate(indices[0]):\n",
    "        if idx < len(documents):  # Sprawdzenie, czy indeks jest prawidłowy\n",
    "            doc = documents[idx].copy()\n",
    "            doc[\"cosine_similarity\"] = float(similarities[0][i])  # Podobieństwo kosinusowe\n",
    "            doc[\"distance\"] = 1.0 - doc[\"cosine_similarity\"]  # Dystans kosinusowy\n",
    "            retrieved_docs.append(doc)\n",
    "    \n",
    "    return retrieved_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f90f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wyszukujemy tutaj dokument dla zapytania identycznego z jednym z dokumentów z bazy\n",
    "# Odnaleziony dokument ma cosine_similarity = 1.0, a więc potwierdzamy, że to co FAISS zwrócił jako distance, w rzeczywistości jest podobieństwem kosinusowym\n",
    "retrieve_documents_vector(\"Laravel to framework aplikacji webowych z elegancką składnią dla PHP, ułatwiający typowe zadania programistyczne.\", documents, embedding_model, index, top_k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36feae54",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieve_documents_vector(\"jakie modele stworzyło OpenAI?\", documents, embedding_model, index, top_k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d4f922",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieve_documents_vector(\"Czym mogę stworzyć bezpieczne połączenie przez Internet?\", documents, embedding_model, index, top_k=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2772089",
   "metadata": {},
   "source": [
    "#### Generowanie odpowiedzi na podstawie wyszukanych dokumentów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d845e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer_vector(query, retrieved_docs):\n",
    "    \"\"\"\n",
    "    Generuje odpowiedź na podstawie zapytania i wyszukanych dokumentów.\n",
    "    \n",
    "    Args:\n",
    "        query: Zapytanie użytkownika\n",
    "        retrieved_docs: Lista wyszukanych dokumentów z ocenami podobieństwa\n",
    "        \n",
    "    Returns:\n",
    "        Wygenerowana odpowiedź\n",
    "    \"\"\"\n",
    "    # Przygotowanie kontekstu dla modelu LLM\n",
    "    context = \"\\n\\n\".join([\n",
    "        f\"Dokument {doc['id']}: {doc['content']}\\n\"\n",
    "        f\"Źródło: {doc['source']}\\n\"\n",
    "        f\"Ocena podobieństwa: {doc['cosine_similarity']:.2f}\"\n",
    "        for doc in retrieved_docs\n",
    "    ])\n",
    "    \n",
    "    # Instrukcje dla modelu\n",
    "    system_prompt = f\"\"\"\n",
    "    Jesteś pomocnym asystentem, który odpowiada na pytania użytkownika w oparciu o dostarczone konteksty.\n",
    "    Twoim zadaniem jest:\n",
    "    1. Przeanalizować dostarczone dokumenty i wybrać te, które są najbardziej istotne dla pytania.\n",
    "    2. Sformułować zwięzłą, ale kompletną odpowiedź opartą na tych dokumentach.\n",
    "    3. Gdy informacje są niepełne lub niepewne, zaznaczyć to w odpowiedzi.\n",
    "    4. Podać źródła użytych informacji w formie odnośników [1], [2], itp. na końcu odpowiedzi, gdzie numer odpowiada ID dokumentu.\n",
    "    \n",
    "    Odpowiedź powinna być napisana w języku polskim, prosta do zrozumienia i dobrze zorganizowana.\n",
    "    \"\"\"\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    Pytanie: {query}\n",
    "    ---\n",
    "    Kontekst:\n",
    "    {context}\n",
    "    \"\"\"\n",
    "    \n",
    "    # Przygotowanie wiadomości dla API\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    \n",
    "    # Wywołanie API\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=messages,\n",
    "        temperature=0.2,  # Niska temperatura dla bardziej deterministycznych odpowiedzi\n",
    "        max_tokens=10000  # Limit długości odpowiedzi\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3201231",
   "metadata": {},
   "source": [
    "#### Funkcja główna łącząca wszystkie komponenty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81136151",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_system_vector(query, top_k=3):\n",
    "    \"\"\"\n",
    "    Kompletny system RAG oparty o bazę wektorową.\n",
    "    \n",
    "    Args:\n",
    "        query: Zapytanie użytkownika\n",
    "        documents: Lista dokumentów z metadanymi\n",
    "        top_k: Liczba najbardziej podobnych dokumentów do wykorzystania\n",
    "        \n",
    "    Returns:\n",
    "        Wygenerowana odpowiedź\n",
    "    \"\"\"\n",
    "    print(f\"Zapytanie: {query}\")\n",
    "    print(\"Inicjalizacja bazy wektorowej...\")\n",
    "    \n",
    "    # Wyszukanie podobnych dokumentów\n",
    "    print(\"Wyszukiwanie podobnych dokumentów...\")\n",
    "    start_time = time.time()\n",
    "    retrieved_docs = retrieve_documents_vector(query, documents, embedding_model, index, top_k)\n",
    "    retrieval_time = time.time() - start_time\n",
    "    print(f\"Znaleziono {len(retrieved_docs)} podobnych dokumentów w {retrieval_time:.2f} sekund.\")\n",
    "    \n",
    "    # Generowanie odpowiedzi\n",
    "    print(\"Generowanie odpowiedzi...\")\n",
    "    start_time = time.time()\n",
    "    answer = generate_answer_vector(query, retrieved_docs)\n",
    "    generation_time = time.time() - start_time\n",
    "    print(f\"Odpowiedź wygenerowana w {generation_time:.2f} sekund.\")\n",
    "    \n",
    "    # Informacje o źródłach\n",
    "    print(\"\\nŹródła informacji:\")\n",
    "    for doc in retrieved_docs:\n",
    "        print(f\"[{doc['id']}] (Ocena podobieństwa: {doc['cosine_similarity']:.2f}) - {doc['content']} \")\n",
    "    \n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9186ffbf",
   "metadata": {},
   "source": [
    "### Testowanie systemu RAG opartego o bazę wektorową"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16980566",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_vector = \"Czym jest deep learning i jak się różni od tradycyjnego uczenia maszynowego?\"\n",
    "answer_vector = rag_system_vector(query_vector, top_k=5)\n",
    "display_answer(answer_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901057be",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_vector = \"Jaki jest najnowszy model OpenAI i co potrafi?\"\n",
    "answer_vector = rag_system_vector(query_vector, top_k=5)\n",
    "display_answer(answer_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf37d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_vector = \"Czym mogę stworzyć bezpieczne połączenie przez Internet?\"\n",
    "answer_vector = rag_system_vector(query_vector, top_k=5)\n",
    "display_answer(answer_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ad850e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
